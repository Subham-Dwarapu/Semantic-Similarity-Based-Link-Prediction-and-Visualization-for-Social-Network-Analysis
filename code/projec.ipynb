{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 111/111 [02:23<00:00,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessing Complete!\n",
      "Processed graphs are stored in: processed_training_graphs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Directories\n",
    "TRAINING_DIR = \"Training_data\"  # Directory containing training .net files\n",
    "PROCESSED_DIR = \"processed_training_graphs\"  # Where preprocessed graphs (.gml) will be stored\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "### **Step 1: Convert .net to .gml and Preprocess Graphs**\n",
    "def preprocess_graph(file_path, output_path):\n",
    "    \"\"\"Converts .net to .gml, removes isolated nodes, and normalizes labels.\"\"\"\n",
    "    G = nx.read_pajek(file_path)  # Load .net graph\n",
    "    G = nx.Graph(G)  # Convert to undirected (if needed)\n",
    "    G.remove_nodes_from(list(nx.isolates(G)))  # Remove isolated nodes\n",
    "\n",
    "    # Convert node labels to integers (required for embeddings)\n",
    "    G = nx.convert_node_labels_to_integers(G, label_attribute=\"original_label\")\n",
    "\n",
    "    # Save as .gml for better compatibility\n",
    "    nx.write_gml(G, output_path)\n",
    "\n",
    "### **Step 2: Process All Graphs in Training Data**\n",
    "for file in tqdm(os.listdir(TRAINING_DIR)):\n",
    "    if file.endswith(\".net\"):\n",
    "        file_path = os.path.join(TRAINING_DIR, file)\n",
    "        output_file = os.path.join(PROCESSED_DIR, file.replace(\".net\", \".gml\"))\n",
    "\n",
    "        # Convert & Preprocess\n",
    "        preprocess_graph(file_path, output_file)\n",
    "\n",
    "print(\"Preprocessing Complete!\")\n",
    "print(f\"Processed graphs are stored in: {PROCESSED_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "  0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Processing ADV_train_0.gml -> Detected as ADV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 4907/4907 [00:11<00:00, 439.92it/s] \n",
      "  1%|          | 1/110 [02:10<3:56:35, 130.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\ADV_train_0_cosine_similarity.csv\n",
      "🔍 Processing ADV_train_1.gml -> Detected as ADV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 4891/4891 [00:11<00:00, 422.33it/s] \n",
      "  2%|▏         | 2/110 [04:18<3:52:37, 129.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\ADV_train_1_cosine_similarity.csv\n",
      "🔍 Processing ADV_train_2.gml -> Detected as ADV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 4903/4903 [00:11<00:00, 424.40it/s] \n",
      "  3%|▎         | 3/110 [06:26<3:48:59, 128.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\ADV_train_2_cosine_similarity.csv\n",
      "🔍 Processing ADV_train_3.gml -> Detected as ADV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 4914/4914 [00:11<00:00, 414.22it/s] \n",
      "  4%|▎         | 4/110 [08:34<3:46:58, 128.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\ADV_train_3_cosine_similarity.csv\n",
      "🔍 Processing ADV_train_4.gml -> Detected as ADV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 4878/4878 [00:11<00:00, 408.78it/s] \n",
      "  5%|▍         | 5/110 [10:39<3:42:45, 127.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\ADV_train_4_cosine_similarity.csv\n",
      "🔍 Processing BUP_train_0.gml -> Detected as BUP\n",
      "🔍 Processing BUP_train_1.gml -> Detected as BUP\n",
      "🔍 Processing BUP_train_2.gml -> Detected as BUP\n",
      "🔍 Processing BUP_train_3.gml -> Detected as BUP\n",
      "🔍 Processing BUP_train_4.gml -> Detected as BUP\n",
      "🔍 Processing CDM_train_0.gml -> Detected as CDM\n",
      "🔍 Processing CDM_train_1.gml -> Detected as CDM\n",
      "🔍 Processing CDM_train_2.gml -> Detected as CDM\n",
      "🔍 Processing CDM_train_3.gml -> Detected as CDM\n",
      "🔍 Processing CDM_train_4.gml -> Detected as CDM\n",
      "🔍 Processing CEG_train_0.gml -> Detected as CEG\n",
      "🔍 Processing CEG_train_1.gml -> Detected as CEG\n",
      "🔍 Processing CEG_train_2.gml -> Detected as CEG\n",
      "🔍 Processing CEG_train_3.gml -> Detected as CEG\n",
      "🔍 Processing CEG_train_4.gml -> Detected as CEG\n",
      "🔍 Processing CGS_train_0.gml -> Detected as CGS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 5648/5648 [00:00<00:00, 7453.96it/s]\n",
      " 19%|█▉        | 21/110 [12:29<29:29, 19.88s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\CGS_train_0_cosine_similarity.csv\n",
      "🔍 Processing CGS_train_1.gml -> Detected as CGS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 5726/5726 [00:00<00:00, 6922.46it/s]\n",
      " 20%|██        | 22/110 [15:42<47:56, 32.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\CGS_train_1_cosine_similarity.csv\n",
      "🔍 Processing CGS_train_2.gml -> Detected as CGS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 5698/5698 [00:02<00:00, 2075.16it/s]\n",
      " 21%|██        | 23/110 [18:46<1:08:15, 47.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\CGS_train_2_cosine_similarity.csv\n",
      "🔍 Processing CGS_train_3.gml -> Detected as CGS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 5664/5664 [00:01<00:00, 5129.86it/s]\n",
      " 22%|██▏       | 24/110 [20:53<1:21:10, 56.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\CGS_train_3_cosine_similarity.csv\n",
      "🔍 Processing CGS_train_4.gml -> Detected as CGS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 5668/5668 [00:00<00:00, 10198.02it/s]\n",
      " 23%|██▎       | 25/110 [23:02<1:35:14, 67.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\CGS_train_4_cosine_similarity.csv\n",
      "🔍 Processing EML_train_0.gml -> Detected as EML\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 1087/1087 [00:00<00:00, 3116.04it/s]\n",
      " 24%|██▎       | 26/110 [23:26<1:23:37, 59.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\EML_train_0_cosine_similarity.csv\n",
      "🔍 Processing EML_train_1.gml -> Detected as EML\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 1102/1102 [00:00<00:00, 3534.64it/s]\n",
      " 25%|██▍       | 27/110 [23:46<1:11:45, 51.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\EML_train_1_cosine_similarity.csv\n",
      "🔍 Processing EML_train_2.gml -> Detected as EML\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 1099/1099 [00:00<00:00, 3691.33it/s]\n",
      " 25%|██▌       | 28/110 [24:09<1:02:12, 45.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\EML_train_2_cosine_similarity.csv\n",
      "🔍 Processing EML_train_3.gml -> Detected as EML\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 1092/1092 [00:00<00:00, 2717.00it/s]\n",
      " 26%|██▋       | 29/110 [24:31<53:55, 39.94s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\EML_train_3_cosine_similarity.csv\n",
      "🔍 Processing EML_train_4.gml -> Detected as EML\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 1103/1103 [00:00<00:00, 3064.46it/s]\n",
      " 27%|██▋       | 30/110 [24:59<49:09, 36.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\EML_train_4_cosine_similarity.csv\n",
      "🔍 Processing ERD_train_0.gml -> Detected as ERD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 5980/5980 [00:02<00:00, 2033.78it/s]\n",
      " 28%|██▊       | 31/110 [29:17<2:06:06, 95.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\ERD_train_0_cosine_similarity.csv\n",
      "🔍 Processing ERD_train_1.gml -> Detected as ERD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 5904/5904 [00:01<00:00, 3276.88it/s]\n",
      " 29%|██▉       | 32/110 [33:05<2:52:08, 132.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\ERD_train_1_cosine_similarity.csv\n",
      "🔍 Processing ERD_train_2.gml -> Detected as ERD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 5953/5953 [00:03<00:00, 1878.73it/s]\n",
      " 30%|███       | 33/110 [37:17<3:33:22, 166.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\ERD_train_2_cosine_similarity.csv\n",
      "🔍 Processing ERD_train_3.gml -> Detected as ERD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 5926/5926 [00:03<00:00, 1779.31it/s]\n",
      " 31%|███       | 34/110 [41:09<3:54:22, 185.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\ERD_train_3_cosine_similarity.csv\n",
      "🔍 Processing ERD_train_4.gml -> Detected as ERD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 5885/5885 [00:03<00:00, 1625.35it/s]\n",
      " 32%|███▏      | 35/110 [45:05<4:09:46, 199.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\ERD_train_4_cosine_similarity.csv\n",
      "🔍 Processing FBK_train_0.gml -> Detected as FBK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 4001/4001 [00:52<00:00, 76.20it/s] \n",
      " 33%|███▎      | 36/110 [48:35<4:10:15, 202.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\FBK_train_0_cosine_similarity.csv\n",
      "🔍 Processing FBK_train_1.gml -> Detected as FBK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 4003/4003 [00:22<00:00, 174.69it/s]\n",
      " 34%|███▎      | 37/110 [50:42<3:39:33, 180.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\FBK_train_1_cosine_similarity.csv\n",
      "🔍 Processing FBK_train_2.gml -> Detected as FBK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 3998/3998 [00:22<00:00, 175.72it/s]\n",
      " 35%|███▍      | 38/110 [52:49<3:17:39, 164.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\FBK_train_2_cosine_similarity.csv\n",
      "🔍 Processing FBK_train_3.gml -> Detected as FBK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 4003/4003 [00:23<00:00, 167.56it/s]\n",
      " 35%|███▌      | 39/110 [54:59<3:02:30, 154.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\FBK_train_3_cosine_similarity.csv\n",
      "🔍 Processing FBK_train_4.gml -> Detected as FBK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 4006/4006 [00:22<00:00, 175.60it/s]\n",
      " 36%|███▋      | 40/110 [57:07<2:50:53, 146.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\FBK_train_4_cosine_similarity.csv\n",
      "🔍 Processing GRQ_train_0.gml -> Detected as GRQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 4942/4942 [00:00<00:00, 6608.77it/s]\n",
      " 37%|███▋      | 41/110 [58:55<2:35:07, 134.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\GRQ_train_0_cosine_similarity.csv\n",
      "🔍 Processing GRQ_train_1.gml -> Detected as GRQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 4945/4945 [00:00<00:00, 7607.39it/s]\n",
      " 38%|███▊      | 42/110 [1:00:43<2:23:57, 127.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\GRQ_train_1_cosine_similarity.csv\n",
      "🔍 Processing GRQ_train_2.gml -> Detected as GRQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 4960/4960 [00:00<00:00, 6617.96it/s]\n",
      " 39%|███▉      | 43/110 [1:02:33<2:16:05, 121.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\GRQ_train_2_cosine_similarity.csv\n",
      "🔍 Processing GRQ_train_3.gml -> Detected as GRQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 4973/4973 [00:00<00:00, 6901.63it/s]\n",
      " 40%|████      | 44/110 [1:04:22<2:09:36, 117.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\GRQ_train_3_cosine_similarity.csv\n",
      "🔍 Processing GRQ_train_4.gml -> Detected as GRQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 4941/4941 [00:00<00:00, 7291.88it/s]\n",
      " 41%|████      | 45/110 [1:06:10<2:04:41, 115.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\GRQ_train_4_cosine_similarity.csv\n",
      "🔍 Processing HMT_train_0.gml -> Detected as HMT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 2340/2340 [00:01<00:00, 1230.25it/s]\n",
      " 42%|████▏     | 46/110 [1:07:01<1:42:13, 95.83s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\HMT_train_0_cosine_similarity.csv\n",
      "🔍 Processing HMT_train_1.gml -> Detected as HMT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 2361/2361 [00:01<00:00, 1284.36it/s]\n",
      " 43%|████▎     | 47/110 [1:07:53<1:26:37, 82.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\HMT_train_1_cosine_similarity.csv\n",
      "🔍 Processing HMT_train_2.gml -> Detected as HMT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 2359/2359 [00:01<00:00, 1274.64it/s]\n",
      " 44%|████▎     | 48/110 [1:08:46<1:16:20, 73.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\HMT_train_2_cosine_similarity.csv\n",
      "🔍 Processing HMT_train_3.gml -> Detected as HMT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 2369/2369 [00:01<00:00, 1273.43it/s]\n",
      " 45%|████▍     | 49/110 [1:09:38<1:08:15, 67.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\HMT_train_3_cosine_similarity.csv\n",
      "🔍 Processing HMT_train_4.gml -> Detected as HMT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 2339/2339 [00:01<00:00, 1257.54it/s]\n",
      " 45%|████▌     | 50/110 [1:10:28<1:02:10, 62.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\HMT_train_4_cosine_similarity.csv\n",
      "🔍 Processing HPD_train_0.gml -> Detected as HPD\n",
      "🔍 Processing HPD_train_1.gml -> Detected as HPD\n",
      "🔍 Processing HPD_train_2.gml -> Detected as HPD\n",
      "🔍 Processing HPD_train_3.gml -> Detected as HPD\n",
      "🔍 Processing HPD_train_4.gml -> Detected as HPD\n",
      "🔍 Processing HTC_train_0.gml -> Detected as HTC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 7174/7174 [00:00<00:00, 12657.72it/s]\n",
      " 51%|█████     | 56/110 [1:13:31<35:26, 39.38s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\HTC_train_0_cosine_similarity.csv\n",
      "🔍 Processing HTC_train_1.gml -> Detected as HTC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 7181/7181 [00:00<00:00, 14220.89it/s]\n",
      " 52%|█████▏    | 57/110 [1:16:33<53:10, 60.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\HTC_train_1_cosine_similarity.csv\n",
      "🔍 Processing HTC_train_2.gml -> Detected as HTC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 7167/7167 [00:00<00:00, 13432.54it/s]\n",
      " 53%|█████▎    | 58/110 [1:19:38<1:10:54, 81.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\HTC_train_2_cosine_similarity.csv\n",
      "🔍 Processing HTC_train_3.gml -> Detected as HTC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 7171/7171 [00:00<00:00, 14385.48it/s]\n",
      " 54%|█████▎    | 59/110 [1:22:39<1:26:09, 101.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\HTC_train_3_cosine_similarity.csv\n",
      "🔍 Processing HTC_train_4.gml -> Detected as HTC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 7158/7158 [00:00<00:00, 12821.41it/s]\n",
      " 55%|█████▍    | 60/110 [1:25:40<1:39:05, 118.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\HTC_train_4_cosine_similarity.csv\n",
      "🔍 Processing INF_train_0.gml -> Detected as INF\n",
      "🔍 Processing INF_train_1.gml -> Detected as INF\n",
      "🔍 Processing INF_train_2.gml -> Detected as INF\n",
      "🔍 Processing INF_train_3.gml -> Detected as INF\n",
      "🔍 Processing INF_train_4.gml -> Detected as INF\n",
      "🔍 Processing KHN_train_0.gml -> Detected as KHN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 3595/3595 [00:02<00:00, 1553.37it/s]\n",
      " 60%|██████    | 66/110 [1:27:09<37:17, 50.84s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\KHN_train_0_cosine_similarity.csv\n",
      "🔍 Processing KHN_train_1.gml -> Detected as KHN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 3629/3629 [00:02<00:00, 1683.84it/s]\n",
      " 61%|██████    | 67/110 [1:28:38<40:09, 56.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\KHN_train_1_cosine_similarity.csv\n",
      "🔍 Processing KHN_train_2.gml -> Detected as KHN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 3595/3595 [00:02<00:00, 1637.92it/s]\n",
      " 62%|██████▏   | 68/110 [1:30:06<42:48, 61.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\KHN_train_2_cosine_similarity.csv\n",
      "🔍 Processing KHN_train_3.gml -> Detected as KHN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 3592/3592 [00:02<00:00, 1611.45it/s]\n",
      " 63%|██████▎   | 69/110 [1:31:36<45:28, 66.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\KHN_train_3_cosine_similarity.csv\n",
      "🔍 Processing KHN_train_4.gml -> Detected as KHN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 3623/3623 [00:02<00:00, 1684.46it/s]\n",
      " 64%|██████▎   | 70/110 [1:33:04<47:23, 71.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\KHN_train_4_cosine_similarity.csv\n",
      "🔍 Processing LDG_train_0.gml -> Detected as LDG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 8062/8062 [00:06<00:00, 1171.25it/s]\n",
      " 65%|██████▍   | 71/110 [1:37:20<1:14:05, 113.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\LDG_train_0_cosine_similarity.csv\n",
      "🔍 Processing LDG_train_1.gml -> Detected as LDG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 8024/8024 [00:07<00:00, 1138.67it/s]\n",
      " 65%|██████▌   | 72/110 [1:41:35<1:34:20, 148.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\LDG_train_1_cosine_similarity.csv\n",
      "🔍 Processing LDG_train_2.gml -> Detected as LDG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 8061/8061 [00:07<00:00, 1105.68it/s]\n",
      " 66%|██████▋   | 73/110 [1:45:52<1:49:27, 177.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\LDG_train_2_cosine_similarity.csv\n",
      "🔍 Processing LDG_train_3.gml -> Detected as LDG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 8068/8068 [00:07<00:00, 1095.49it/s]\n",
      " 67%|██████▋   | 74/110 [1:50:10<1:59:33, 199.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\LDG_train_3_cosine_similarity.csv\n",
      "🔍 Processing LDG_train_4.gml -> Detected as LDG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 8054/8054 [00:07<00:00, 1055.75it/s]\n",
      " 68%|██████▊   | 75/110 [1:54:27<2:05:37, 215.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\LDG_train_4_cosine_similarity.csv\n",
      "🔍 Processing NSC_train_0.gml -> Detected as NSC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 1382/1382 [00:00<00:00, 18522.93it/s]\n",
      " 69%|██████▉   | 76/110 [1:54:45<1:30:12, 159.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\NSC_train_0_cosine_similarity.csv\n",
      "🔍 Processing NSC_train_1.gml -> Detected as NSC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 1382/1382 [00:00<00:00, 18958.64it/s]\n",
      " 70%|███████   | 77/110 [1:55:04<1:05:09, 118.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\NSC_train_1_cosine_similarity.csv\n",
      "🔍 Processing NSC_train_2.gml -> Detected as NSC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 1388/1388 [00:00<00:00, 17176.79it/s]\n",
      " 71%|███████   | 78/110 [1:55:23<47:35, 89.23s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\NSC_train_2_cosine_similarity.csv\n",
      "🔍 Processing NSC_train_3.gml -> Detected as NSC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 1387/1387 [00:00<00:00, 16888.07it/s]\n",
      " 72%|███████▏  | 79/110 [1:55:41<35:19, 68.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\NSC_train_3_cosine_similarity.csv\n",
      "🔍 Processing NSC_train_4.gml -> Detected as NSC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 1370/1370 [00:00<00:00, 17523.11it/s]\n",
      " 73%|███████▎  | 80/110 [1:56:00<26:51, 53.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\NSC_train_4_cosine_similarity.csv\n",
      "🔍 Processing PGP_train_0.gml -> Detected as PGP\n",
      "🔍 Processing PGP_train_1.gml -> Detected as PGP\n",
      "🔍 Processing PGP_train_2.gml -> Detected as PGP\n",
      "🔍 Processing PGP_train_3.gml -> Detected as PGP\n",
      "🔍 Processing PGP_train_4.gml -> Detected as PGP\n",
      "🔍 Processing SMG_train_0.gml -> Detected as SMG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 992/992 [00:00<00:00, 1997.58it/s]\n",
      " 78%|███████▊  | 86/110 [1:56:20<07:01, 17.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\SMG_train_0_cosine_similarity.csv\n",
      "🔍 Processing SMG_train_1.gml -> Detected as SMG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 999/999 [00:00<00:00, 1681.70it/s]\n",
      " 79%|███████▉  | 87/110 [1:56:40<06:51, 17.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\SMG_train_1_cosine_similarity.csv\n",
      "🔍 Processing SMG_train_2.gml -> Detected as SMG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 992/992 [00:00<00:00, 1631.50it/s]\n",
      " 80%|████████  | 88/110 [1:57:00<06:40, 18.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\SMG_train_2_cosine_similarity.csv\n",
      "🔍 Processing SMG_train_3.gml -> Detected as SMG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 996/996 [00:00<00:00, 1789.44it/s]\n",
      " 81%|████████  | 89/110 [1:57:20<06:28, 18.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\SMG_train_3_cosine_similarity.csv\n",
      "🔍 Processing SMG_train_4.gml -> Detected as SMG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 997/997 [00:00<00:00, 2036.38it/s]\n",
      " 82%|████████▏ | 90/110 [1:57:39<06:15, 18.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\SMG_train_4_cosine_similarity.csv\n",
      "🔍 Processing UAL_train_0.gml -> Detected as UAL\n",
      "🔍 Processing UAL_train_1.gml -> Detected as UAL\n",
      "🔍 Processing UAL_train_2.gml -> Detected as UAL\n",
      "🔍 Processing UAL_train_3.gml -> Detected as UAL\n",
      "🔍 Processing UAL_train_4.gml -> Detected as UAL\n",
      "🔍 Processing UPG_train_0.gml -> Detected as UPG\n",
      "🔍 Processing UPG_train_1.gml -> Detected as UPG\n",
      "🔍 Processing UPG_train_2.gml -> Detected as UPG\n",
      "🔍 Processing UPG_train_3.gml -> Detected as UPG\n",
      "🔍 Processing UPG_train_4.gml -> Detected as UPG\n",
      "🔍 Processing YST_train_0.gml -> Detected as YST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 2112/2112 [00:00<00:00, 6200.56it/s]\n",
      " 92%|█████████▏| 101/110 [1:58:20<01:04,  7.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\YST_train_0_cosine_similarity.csv\n",
      "🔍 Processing YST_train_1.gml -> Detected as YST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 2121/2121 [00:00<00:00, 5890.25it/s]\n",
      " 93%|█████████▎| 102/110 [1:59:03<01:22, 10.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\YST_train_1_cosine_similarity.csv\n",
      "🔍 Processing YST_train_2.gml -> Detected as YST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 2124/2124 [00:00<00:00, 5383.52it/s]\n",
      " 94%|█████████▎| 103/110 [1:59:44<01:37, 13.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\YST_train_2_cosine_similarity.csv\n",
      "🔍 Processing YST_train_3.gml -> Detected as YST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 2110/2110 [00:00<00:00, 6353.01it/s]\n",
      " 95%|█████████▍| 104/110 [2:00:26<01:47, 17.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\YST_train_3_cosine_similarity.csv\n",
      "🔍 Processing YST_train_4.gml -> Detected as YST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 2127/2127 [00:00<00:00, 6443.83it/s]\n",
      " 95%|█████████▌| 105/110 [2:01:09<01:50, 22.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\YST_train_4_cosine_similarity.csv\n",
      "🔍 Processing ZWL_train_0.gml -> Detected as ZWL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 6568/6568 [00:06<00:00, 1049.80it/s]\n",
      " 96%|█████████▋| 106/110 [2:04:33<03:49, 57.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\ZWL_train_0_cosine_similarity.csv\n",
      "🔍 Processing ZWL_train_1.gml -> Detected as ZWL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 6542/6542 [00:06<00:00, 1041.16it/s]\n",
      " 97%|█████████▋| 107/110 [2:07:53<04:25, 88.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\ZWL_train_1_cosine_similarity.csv\n",
      "🔍 Processing ZWL_train_2.gml -> Detected as ZWL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 6566/6566 [00:06<00:00, 1028.46it/s]\n",
      " 98%|█████████▊| 108/110 [2:11:18<03:51, 115.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\ZWL_train_2_cosine_similarity.csv\n",
      "🔍 Processing ZWL_train_3.gml -> Detected as ZWL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 6554/6554 [00:06<00:00, 1019.96it/s]\n",
      " 99%|█████████▉| 109/110 [2:14:49<02:20, 140.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\ZWL_train_3_cosine_similarity.csv\n",
      "🔍 Processing ZWL_train_4.gml -> Detected as ZWL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 6548/6548 [00:06<00:00, 969.33it/s] \n",
      "100%|██████████| 110/110 [2:18:14<00:00, 75.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved similarity: node2vec_embeddings\\ZWL_train_4_cosine_similarity.csv\n",
      "\n",
      "🚀 Cosine similarity computed for all networks!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from node2vec import Node2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Directories\n",
    "PROCESSED_DIR = \"processed_training_graphs\"  # Input graphs\n",
    "OUTPUT_DIR = \"node2vec_embeddings\"  # Stores only cosine similarity matrices\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Networks to process\n",
    "NODE2VEC_NETWORKS = {\n",
    "    \"ERD\", \"KHN\", \"LDG\", \"SMG\", \"ZWL\", \"HTC\", \"CGS\",\n",
    "    \"NSC\", \"GRQ\", \"HMT\", \"FBK\", \"ADV\", \"EML\", \"YST\"\n",
    "}\n",
    "\n",
    "### **Step 1: Generate Node2Vec Embeddings**\n",
    "def generate_node2vec_embeddings(graph, dimensions=128, walk_length=80, num_walks=10, p=1, q=1):\n",
    "    \"\"\"Generates Node2Vec embeddings for a given NetworkX graph.\"\"\"\n",
    "    node2vec = Node2Vec(graph, dimensions=dimensions, walk_length=walk_length, num_walks=num_walks, p=p, q=q, workers=4)\n",
    "    model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "    return model\n",
    "\n",
    "### **Step 2: Compute Cosine Similarity**\n",
    "def compute_similarity(embeddings):\n",
    "    \"\"\"Computes cosine similarity with proper diagonal values and symmetry.\"\"\"\n",
    "    # Ensure no NaN values in embeddings\n",
    "    if np.isnan(embeddings).any():\n",
    "        # Replace NaN values with zeros - or consider better handling\n",
    "        embeddings = np.nan_to_num(embeddings, nan=0.0)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarity = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Ensure proper diagonal values (should be 1.0)\n",
    "    np.fill_diagonal(similarity, 1.0)\n",
    "    \n",
    "    # Ensure symmetry by averaging with transpose\n",
    "    # This makes sim(i,j) = sim(j,i)\n",
    "    similarity = (similarity + similarity.T) / 2\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "# Process all .gml files\n",
    "for file in tqdm(os.listdir(PROCESSED_DIR)):\n",
    "    if file.endswith(\".gml\"):\n",
    "        base_name = file.split(\"_train_\")[0]\n",
    "        print(f\"Processing {file} -> Detected as {base_name}\")\n",
    "\n",
    "        if base_name in NODE2VEC_NETWORKS:\n",
    "            file_path = os.path.join(PROCESSED_DIR, file)\n",
    "\n",
    "            # Load Graph\n",
    "            G = nx.read_gml(file_path)\n",
    "\n",
    "            # Skip very small graphs\n",
    "            if len(G.nodes) < 5:\n",
    "                print(f\"Skipping {file} (Graph too small: {len(G.nodes)} nodes)\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Generate Node2Vec Embeddings\n",
    "                model = generate_node2vec_embeddings(G)\n",
    "            except Exception as e:\n",
    "                print(f\"Node2Vec failed for {file}: {e}\")\n",
    "                continue  # Move to the next network\n",
    "\n",
    "            # Extract embeddings\n",
    "            try:\n",
    "                # Get node list first to ensure order consistency\n",
    "                node_labels = list(G.nodes())\n",
    "                \n",
    "                # More robust embedding extraction with error handling\n",
    "                node_embeddings = []\n",
    "                for node in node_labels:\n",
    "                    try:\n",
    "                        node_embeddings.append(model.wv[str(node)])\n",
    "                    except KeyError:\n",
    "                        print(f\"Missing embedding for node {node} in {file}, using zeros\")\n",
    "                        # Use zero vector for missing embeddings\n",
    "                        node_embeddings.append(np.zeros(model.wv.vector_size))\n",
    "                \n",
    "                node_embeddings = np.array(node_embeddings)\n",
    "                \n",
    "                # Validate embeddings - check for zero-vectors that would cause NaN\n",
    "                zero_vectors = np.where(np.all(node_embeddings == 0, axis=1))[0]\n",
    "                if len(zero_vectors) > 0:\n",
    "                    print(f\"Found {len(zero_vectors)} zero vectors in {file}\")\n",
    "                    # Replace zeros with small random values to avoid NaN in cosine similarity\n",
    "                    for idx in zero_vectors:\n",
    "                        node_embeddings[idx] = np.random.normal(0, 0.01, model.wv.vector_size)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Embedding extraction failed for {file}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Compute and Save Cosine Similarity\n",
    "            try:\n",
    "                similarity_matrix = compute_similarity(node_embeddings)\n",
    "                similarity_df = pd.DataFrame(similarity_matrix, index=node_labels, columns=node_labels)\n",
    "\n",
    "                similarity_filename = file.replace(\".gml\", \"_cosine_similarity.csv\")\n",
    "                similarity_path = os.path.join(OUTPUT_DIR, similarity_filename)\n",
    "                similarity_df.to_csv(similarity_path)\n",
    "\n",
    "                print(f\"Saved similarity: {similarity_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Similarity computation failed for {file}: {e}\")\n",
    "\n",
    "print(\"\\nCosine similarity computed for all networks!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing CDM_train_0.gml...\n",
      "Using largest connected component with 13290 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_2960\\2760154915.py:36: DeprecationWarning: `asfptype` is an internal function, and is deprecated as part of the public API. It will be removed in v1.14.0.\n",
      "  laplacian = nx.normalized_laplacian_matrix(G).asfptype()\n",
      "  5%|▌         | 1/20 [04:46<1:30:49, 286.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed CDM_train_0.gml with spectral\n",
      "Processing CDM_train_1.gml...\n",
      "Using largest connected component with 13244 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_2960\\2760154915.py:36: DeprecationWarning: `asfptype` is an internal function, and is deprecated as part of the public API. It will be removed in v1.14.0.\n",
      "  laplacian = nx.normalized_laplacian_matrix(G).asfptype()\n",
      " 10%|█         | 2/20 [09:52<1:29:22, 297.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed CDM_train_1.gml with spectral\n",
      "Processing CDM_train_2.gml...\n",
      "Using largest connected component with 13247 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_2960\\2760154915.py:36: DeprecationWarning: `asfptype` is an internal function, and is deprecated as part of the public API. It will be removed in v1.14.0.\n",
      "  laplacian = nx.normalized_laplacian_matrix(G).asfptype()\n",
      " 15%|█▌        | 3/20 [20:16<2:06:32, 446.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed CDM_train_2.gml with spectral\n",
      "Processing CDM_train_3.gml...\n",
      "Using largest connected component with 13250 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_2960\\2760154915.py:36: DeprecationWarning: `asfptype` is an internal function, and is deprecated as part of the public API. It will be removed in v1.14.0.\n",
      "  laplacian = nx.normalized_laplacian_matrix(G).asfptype()\n",
      " 20%|██        | 4/20 [29:32<2:10:39, 489.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed CDM_train_3.gml with spectral\n",
      "Processing CDM_train_4.gml...\n",
      "Using largest connected component with 13315 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_2960\\2760154915.py:36: DeprecationWarning: `asfptype` is an internal function, and is deprecated as part of the public API. It will be removed in v1.14.0.\n",
      "  laplacian = nx.normalized_laplacian_matrix(G).asfptype()\n",
      " 25%|██▌       | 5/20 [38:07<2:04:45, 499.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed CDM_train_4.gml with spectral\n",
      "Processing HPD_train_0.gml...\n",
      "Using largest connected component with 7976 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_2960\\2760154915.py:36: DeprecationWarning: `asfptype` is an internal function, and is deprecated as part of the public API. It will be removed in v1.14.0.\n",
      "  laplacian = nx.normalized_laplacian_matrix(G).asfptype()\n",
      " 30%|███       | 6/20 [41:44<1:34:02, 403.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed HPD_train_0.gml with spectral\n",
      "Processing HPD_train_1.gml...\n",
      "Using largest connected component with 7952 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_2960\\2760154915.py:36: DeprecationWarning: `asfptype` is an internal function, and is deprecated as part of the public API. It will be removed in v1.14.0.\n",
      "  laplacian = nx.normalized_laplacian_matrix(G).asfptype()\n",
      " 35%|███▌      | 7/20 [45:12<1:13:31, 339.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed HPD_train_1.gml with spectral\n",
      "Processing HPD_train_2.gml...\n",
      "Using largest connected component with 7973 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_2960\\2760154915.py:36: DeprecationWarning: `asfptype` is an internal function, and is deprecated as part of the public API. It will be removed in v1.14.0.\n",
      "  laplacian = nx.normalized_laplacian_matrix(G).asfptype()\n",
      " 40%|████      | 8/20 [48:38<59:22, 296.88s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed HPD_train_2.gml with spectral\n",
      "Processing HPD_train_3.gml...\n",
      "Using largest connected component with 7971 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_2960\\2760154915.py:36: DeprecationWarning: `asfptype` is an internal function, and is deprecated as part of the public API. It will be removed in v1.14.0.\n",
      "  laplacian = nx.normalized_laplacian_matrix(G).asfptype()\n",
      " 45%|████▌     | 9/20 [51:18<46:36, 254.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed HPD_train_3.gml with spectral\n",
      "Processing HPD_train_4.gml...\n",
      "Using largest connected component with 7933 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_2960\\2760154915.py:36: DeprecationWarning: `asfptype` is an internal function, and is deprecated as part of the public API. It will be removed in v1.14.0.\n",
      "  laplacian = nx.normalized_laplacian_matrix(G).asfptype()\n",
      " 50%|█████     | 10/20 [53:34<36:17, 217.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed HPD_train_4.gml with spectral\n",
      "Processing PGP_train_0.gml...\n",
      "Using largest connected component with 8733 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_2960\\2760154915.py:36: DeprecationWarning: `asfptype` is an internal function, and is deprecated as part of the public API. It will be removed in v1.14.0.\n",
      "  laplacian = nx.normalized_laplacian_matrix(G).asfptype()\n",
      " 55%|█████▌    | 11/20 [57:14<32:46, 218.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed PGP_train_0.gml with spectral\n",
      "Processing PGP_train_1.gml...\n",
      "Using largest connected component with 8787 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_2960\\2760154915.py:36: DeprecationWarning: `asfptype` is an internal function, and is deprecated as part of the public API. It will be removed in v1.14.0.\n",
      "  laplacian = nx.normalized_laplacian_matrix(G).asfptype()\n",
      " 60%|██████    | 12/20 [59:31<25:48, 193.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed PGP_train_1.gml with spectral\n",
      "Processing PGP_train_2.gml...\n",
      "Using largest connected component with 8733 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_2960\\2760154915.py:36: DeprecationWarning: `asfptype` is an internal function, and is deprecated as part of the public API. It will be removed in v1.14.0.\n",
      "  laplacian = nx.normalized_laplacian_matrix(G).asfptype()\n",
      " 65%|██████▌   | 13/20 [1:01:50<20:37, 176.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed PGP_train_2.gml with spectral\n",
      "Processing PGP_train_3.gml...\n",
      "Using largest connected component with 8785 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_2960\\2760154915.py:36: DeprecationWarning: `asfptype` is an internal function, and is deprecated as part of the public API. It will be removed in v1.14.0.\n",
      "  laplacian = nx.normalized_laplacian_matrix(G).asfptype()\n",
      " 70%|███████   | 14/20 [1:04:07<16:30, 165.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed PGP_train_3.gml with spectral\n",
      "Processing PGP_train_4.gml...\n",
      "Using largest connected component with 8481 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_2960\\2760154915.py:36: DeprecationWarning: `asfptype` is an internal function, and is deprecated as part of the public API. It will be removed in v1.14.0.\n",
      "  laplacian = nx.normalized_laplacian_matrix(G).asfptype()\n",
      " 75%|███████▌  | 15/20 [1:06:17<12:52, 154.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed PGP_train_4.gml with spectral\n",
      "Processing UPG_train_0.gml...\n",
      "Using largest connected component with 3980 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_2960\\2760154915.py:36: DeprecationWarning: `asfptype` is an internal function, and is deprecated as part of the public API. It will be removed in v1.14.0.\n",
      "  laplacian = nx.normalized_laplacian_matrix(G).asfptype()\n",
      " 80%|████████  | 16/20 [1:06:51<07:52, 118.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed UPG_train_0.gml with spectral\n",
      "Processing UPG_train_1.gml...\n",
      "Using largest connected component with 3876 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_2960\\2760154915.py:36: DeprecationWarning: `asfptype` is an internal function, and is deprecated as part of the public API. It will be removed in v1.14.0.\n",
      "  laplacian = nx.normalized_laplacian_matrix(G).asfptype()\n",
      " 85%|████████▌ | 17/20 [1:07:23<04:36, 92.18s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed UPG_train_1.gml with spectral\n",
      "Processing UPG_train_2.gml...\n",
      "Using largest connected component with 4075 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_2960\\2760154915.py:36: DeprecationWarning: `asfptype` is an internal function, and is deprecated as part of the public API. It will be removed in v1.14.0.\n",
      "  laplacian = nx.normalized_laplacian_matrix(G).asfptype()\n",
      " 90%|█████████ | 18/20 [1:07:59<02:30, 75.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed UPG_train_2.gml with spectral\n",
      "Processing UPG_train_3.gml...\n",
      "Using largest connected component with 3871 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_2960\\2760154915.py:36: DeprecationWarning: `asfptype` is an internal function, and is deprecated as part of the public API. It will be removed in v1.14.0.\n",
      "  laplacian = nx.normalized_laplacian_matrix(G).asfptype()\n",
      " 95%|█████████▌| 19/20 [1:08:32<01:02, 62.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed UPG_train_3.gml with spectral\n",
      "Processing UPG_train_4.gml...\n",
      "Using largest connected component with 4059 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_2960\\2760154915.py:36: DeprecationWarning: `asfptype` is an internal function, and is deprecated as part of the public API. It will be removed in v1.14.0.\n",
      "  laplacian = nx.normalized_laplacian_matrix(G).asfptype()\n",
      "100%|██████████| 20/20 [1:09:07<00:00, 207.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed UPG_train_4.gml with spectral\n",
      "✅ Semantic similarity computation completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Configuration\n",
    "INPUT_DIR = \"processed_training_graph_spectral\"\n",
    "OUTPUT_DIR = \"spectral_similarity_embeddings\"\n",
    "SPARSE_NETWORKS = [\"UPG\", \"HPD\", \"PGP\", \"CDM\"]  # Networks needing spectral embeddings\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --------------------------\n",
    "# Embedding Generation Functions\n",
    "# --------------------------\n",
    "\n",
    "def generate_spectral_embeddings(G, dimensions=32):\n",
    "    \"\"\"Generate spectral embeddings using Laplacian Eigenmaps (optimized for large networks)\"\"\"\n",
    "    try:\n",
    "        if len(G) < 2:\n",
    "            print(\"Graph is too small for spectral embeddings.\")\n",
    "            return None, None\n",
    "\n",
    "        # Use the largest connected component to avoid convergence issues\n",
    "        if not nx.is_connected(G):\n",
    "            largest_cc = max(nx.connected_components(G), key=len)\n",
    "            G = G.subgraph(largest_cc).copy()\n",
    "            print(f\"Using largest connected component with {len(G)} nodes.\")\n",
    "\n",
    "        # Dynamically adjust dimensions\n",
    "        dimensions = min(dimensions, len(G) - 1)\n",
    "        \n",
    "        # Compute normalized Laplacian\n",
    "        laplacian = nx.normalized_laplacian_matrix(G).asfptype()\n",
    "\n",
    "        # Eigen decomposition with increased iterations\n",
    "        eigenvalues, eigenvectors = sp.linalg.eigsh(\n",
    "            laplacian.astype(float), k=dimensions+1, which='SM', maxiter=100000\n",
    "        )\n",
    "\n",
    "        # Discard first eigenvector (trivial solution) and normalize\n",
    "        embeddings = eigenvectors[:, 1:dimensions+1].real\n",
    "        \n",
    "        # Ensure no NaN values in embeddings\n",
    "        if np.isnan(embeddings).any():\n",
    "            print(f\"Warning: NaN values found in embeddings, replacing with zeros\")\n",
    "            embeddings = np.nan_to_num(embeddings, nan=0.0)\n",
    "            \n",
    "        # Normalize embeddings to prevent NaN in cosine similarity\n",
    "        norms = np.linalg.norm(embeddings, axis=1)\n",
    "        \n",
    "        # Handle zero vectors to prevent division by zero\n",
    "        mask = norms > 1e-10\n",
    "        normalized_embeddings = np.zeros_like(embeddings)\n",
    "        normalized_embeddings[mask] = embeddings[mask] / norms[mask, None]\n",
    "        \n",
    "        # Replace zero vectors with small random values\n",
    "        zero_vectors = np.where(~mask)[0]\n",
    "        if len(zero_vectors) > 0:\n",
    "            print(f\"Found {len(zero_vectors)} zero vectors, replacing with small random values\")\n",
    "            for idx in zero_vectors:\n",
    "                normalized_embeddings[idx] = np.random.normal(0, 0.01, dimensions)\n",
    "                # Normalize these random vectors too\n",
    "                normalized_embeddings[idx] = normalized_embeddings[idx] / np.linalg.norm(normalized_embeddings[idx])\n",
    "                \n",
    "        return normalized_embeddings, list(G.nodes())\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Spectral embedding failed: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Modified Processing Pipeline\n",
    "# --------------------------\n",
    "\n",
    "def compute_similarity(embeddings):\n",
    "    \"\"\"Compute cosine similarity with proper diagonal values and symmetry\"\"\"\n",
    "    try:\n",
    "        # Compute cosine similarity\n",
    "        similarity = cosine_similarity(embeddings)\n",
    "        \n",
    "        # Ensure proper diagonal values (should be 1.0)\n",
    "        np.fill_diagonal(similarity, 1.0)\n",
    "        \n",
    "        # Ensure symmetry by averaging with transpose\n",
    "        similarity = (similarity + similarity.T) / 2\n",
    "        \n",
    "        return similarity\n",
    "    except Exception as e:\n",
    "        print(f\"Similarity computation error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Main processing loop\n",
    "for file in tqdm(os.listdir(INPUT_DIR), leave=True, mininterval=1):\n",
    "    if file.endswith(\".gml\"):\n",
    "        file_path = os.path.join(INPUT_DIR, file)\n",
    "        print(f\"Processing {file}...\")\n",
    "        \n",
    "        try:\n",
    "            G = nx.read_gml(file_path)  # Load graph\n",
    "            \n",
    "            # Default to spectral for SPARSE_NETWORKS\n",
    "            method = \"spectral\"\n",
    "            if any(network in file for network in SPARSE_NETWORKS):\n",
    "                embeddings, nodes = generate_spectral_embeddings(G)\n",
    "            else:\n",
    "                # For non-sparse networks, also use spectral but with possibility\n",
    "                # to add different methods in the future\n",
    "                embeddings, nodes = generate_spectral_embeddings(G)\n",
    "                \n",
    "            if embeddings is None or len(embeddings) == 0:\n",
    "                print(f\"Skipping {file} - embedding generation failed\")\n",
    "                continue\n",
    "                \n",
    "            # Validate embeddings before computing similarity\n",
    "            if np.isnan(embeddings).any():\n",
    "                print(f\"Warning: NaN values in final embeddings for {file}, fixing...\")\n",
    "                embeddings = np.nan_to_num(embeddings, nan=0.0)\n",
    "                \n",
    "            # Compute & save similarity\n",
    "            similarity_matrix = compute_similarity(embeddings)\n",
    "            if similarity_matrix is not None:\n",
    "                # Final check for NaN values\n",
    "                if np.isnan(similarity_matrix).any():\n",
    "                    print(f\"Warning: NaN values in similarity matrix for {file}, fixing...\")\n",
    "                    similarity_matrix = np.nan_to_num(similarity_matrix, nan=0.0)\n",
    "                    # Re-ensure diagonal is 1.0\n",
    "                    np.fill_diagonal(similarity_matrix, 1.0)\n",
    "                \n",
    "                df = pd.DataFrame(similarity_matrix, index=nodes, columns=nodes)\n",
    "                output_file = os.path.join(OUTPUT_DIR, f\"{file.replace('.gml', '')}_{method}_cosine_similarity.csv\")\n",
    "                df.to_csv(output_file)\n",
    "                \n",
    "                # Verify the saved matrix (optional)\n",
    "                saved_matrix = pd.read_csv(output_file, index_col=0).values\n",
    "                issues = []\n",
    "                if np.isnan(saved_matrix).any():\n",
    "                    issues.append(\"Contains NaN\")\n",
    "                if not np.allclose(saved_matrix, saved_matrix.T, rtol=1e-5, atol=1e-8):\n",
    "                    issues.append(\"Matrix not symmetric\")\n",
    "                if not np.allclose(np.diag(saved_matrix), np.ones(saved_matrix.shape[0]), rtol=1e-5, atol=1e-8):\n",
    "                    issues.append(\"Diagonal values not 1\")\n",
    "                \n",
    "                if issues:\n",
    "                    print(f\"Issues with {output_file}: {'; '.join(issues)}\")\n",
    "                else:\n",
    "                    print(f\"Successfully processed {file} with {method}\")\n",
    "            else:\n",
    "                print(f\"Similarity computation failed for {file}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {str(e)}\")\n",
    "\n",
    "print(\"Semantic similarity computation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Processing BUP_train_0.gml with Spectral Embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:00<00:02,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved small_network_similarity_results\\BUP_train_0_cosine_similarity.csv\n",
      "🔵 Processing BUP_train_1.gml with Spectral Embeddings\n",
      "✅ Saved small_network_similarity_results\\BUP_train_1_cosine_similarity.csv\n",
      "🔵 Processing BUP_train_2.gml with Spectral Embeddings\n",
      "✅ Saved small_network_similarity_results\\BUP_train_2_cosine_similarity.csv\n",
      "🔵 Processing BUP_train_3.gml with Spectral Embeddings\n",
      "✅ Saved small_network_similarity_results\\BUP_train_3_cosine_similarity.csv\n",
      "🔵 Processing BUP_train_4.gml with Spectral Embeddings\n",
      "✅ Saved small_network_similarity_results\\BUP_train_4_cosine_similarity.csv\n",
      "🔵 Processing CEG_train_0.gml with Spectral Embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:01<00:02,  5.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved small_network_similarity_results\\CEG_train_0_cosine_similarity.csv\n",
      "🔵 Processing CEG_train_1.gml with Spectral Embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [00:01<00:02,  4.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved small_network_similarity_results\\CEG_train_1_cosine_similarity.csv\n",
      "🔵 Processing CEG_train_2.gml with Spectral Embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [00:01<00:02,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved small_network_similarity_results\\CEG_train_2_cosine_similarity.csv\n",
      "🔵 Processing CEG_train_3.gml with Spectral Embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [00:02<00:02,  4.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved small_network_similarity_results\\CEG_train_3_cosine_similarity.csv\n",
      "🔵 Processing CEG_train_4.gml with Spectral Embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [00:02<00:02,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved small_network_similarity_results\\CEG_train_4_cosine_similarity.csv\n",
      "🔵 Processing INF_train_0.gml with Spectral Embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [00:02<00:02,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved small_network_similarity_results\\INF_train_0_cosine_similarity.csv\n",
      "🔵 Processing INF_train_1.gml with Spectral Embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [00:03<00:02,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved small_network_similarity_results\\INF_train_1_cosine_similarity.csv\n",
      "🔵 Processing INF_train_2.gml with Spectral Embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [00:03<00:02,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved small_network_similarity_results\\INF_train_2_cosine_similarity.csv\n",
      "🔵 Processing INF_train_3.gml with Spectral Embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [00:04<00:02,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved small_network_similarity_results\\INF_train_3_cosine_similarity.csv\n",
      "🔵 Processing INF_train_4.gml with Spectral Embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [00:04<00:01,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved small_network_similarity_results\\INF_train_4_cosine_similarity.csv\n",
      "🔵 Processing UAL_train_0.gml with Spectral Embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [00:04<00:01,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved small_network_similarity_results\\UAL_train_0_cosine_similarity.csv\n",
      "🔵 Processing UAL_train_1.gml with Spectral Embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [00:04<00:00,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved small_network_similarity_results\\UAL_train_1_cosine_similarity.csv\n",
      "🔵 Processing UAL_train_2.gml with Spectral Embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [00:05<00:00,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved small_network_similarity_results\\UAL_train_2_cosine_similarity.csv\n",
      "🔵 Processing UAL_train_3.gml with Spectral Embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\Lib\\site-packages\\sklearn\\manifold\\_spectral_embedding.py:285: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
      "  warnings.warn(\n",
      " 95%|█████████▌| 19/20 [00:05<00:00,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved small_network_similarity_results\\UAL_train_3_cosine_similarity.csv\n",
      "🔵 Processing UAL_train_4.gml with Spectral Embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:06<00:00,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved small_network_similarity_results\\UAL_train_4_cosine_similarity.csv\n",
      "\n",
      "🚀 Spectral embeddings and cosine similarity computed for all small networks!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import scipy.sparse as sp\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "\n",
    "# Directories\n",
    "PROCESSED_DIR = \"training_spectral_class\"  # Input directory for processed graphs\n",
    "OUTPUT_DIR = \"small_network_similarity_results\"  # Separate output directory for Group 4\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Group 4: Small-Scale Networks\n",
    "SMALL_NETWORKS = {\"BUP\", \"CEG\", \"INF\", \"UAL\"}\n",
    "\n",
    "def generate_spectral_embeddings(G, dimensions=64):\n",
    "    \"\"\"Generate spectral embeddings using sklearn's SpectralEmbedding (for small graphs).\"\"\"\n",
    "    try:\n",
    "        if len(G) < 2:\n",
    "            print(\"Graph is too small for spectral embeddings.\")\n",
    "            return None, None\n",
    "\n",
    "        # Use sklearn's SpectralEmbedding (better for small graphs)\n",
    "        model = SpectralEmbedding(n_components=min(dimensions, len(G)-1), \n",
    "                                  affinity='precomputed', \n",
    "                                  random_state=42)\n",
    "        adjacency_matrix = nx.to_numpy_array(G)  # Convert graph to adjacency matrix\n",
    "        embeddings = model.fit_transform(adjacency_matrix)\n",
    "        \n",
    "        # Check for and handle NaN values\n",
    "        if np.isnan(embeddings).any():\n",
    "            print(f\"NaN values found in embeddings, replacing with zeros\")\n",
    "            embeddings = np.nan_to_num(embeddings, nan=0.0)\n",
    "        \n",
    "        # Normalize embeddings to unit length for proper cosine similarity\n",
    "        norms = np.linalg.norm(embeddings, axis=1)\n",
    "        zero_norm_indices = np.where(norms < 1e-10)[0]\n",
    "        \n",
    "        # Handle zero vectors to prevent division by zero\n",
    "        if len(zero_norm_indices) > 0:\n",
    "            print(f\"Found {len(zero_norm_indices)} zero vectors, replacing with random unit vectors\")\n",
    "            for idx in zero_norm_indices:\n",
    "                random_vec = np.random.normal(0, 0.01, embeddings.shape[1])\n",
    "                embeddings[idx] = random_vec / np.linalg.norm(random_vec)\n",
    "        else:\n",
    "            # Normalize all non-zero vectors\n",
    "            embeddings = embeddings / norms[:, np.newaxis]\n",
    "            \n",
    "        return embeddings, list(G.nodes())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Spectral embedding failed: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def compute_cosine_similarity(embeddings):\n",
    "    \"\"\"Compute cosine similarity matrix with proper diagonal values and symmetry.\"\"\"\n",
    "    # Compute cosine similarity\n",
    "    similarity = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Ensure proper diagonal values (should be 1.0)\n",
    "    np.fill_diagonal(similarity, 1.0)\n",
    "    \n",
    "    # Ensure symmetry by averaging with transpose\n",
    "    similarity = (similarity + similarity.T) / 2\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "# Process all .gml files and apply Spectral Embeddings for Small Networks\n",
    "for file in tqdm(os.listdir(PROCESSED_DIR)):\n",
    "    if file.endswith(\".gml\"):\n",
    "        base_name = file.split(\"_train_\")[0]\n",
    "        file_path = os.path.join(PROCESSED_DIR, file)\n",
    "        \n",
    "        try:\n",
    "            G = nx.read_gml(file_path)\n",
    "\n",
    "            # Skip very small graphs\n",
    "            if len(G.nodes) < 5:\n",
    "                print(f\"Skipping {file} (Graph too small: {len(G.nodes)} nodes)\")\n",
    "                continue\n",
    "\n",
    "            if base_name in SMALL_NETWORKS:\n",
    "                print(f\"Processing {file} with Spectral Embeddings\")\n",
    "                embeddings, nodes = generate_spectral_embeddings(G)\n",
    "\n",
    "                if embeddings is None or len(embeddings) == 0:\n",
    "                    print(f\"Skipping {file} due to embedding failure\")\n",
    "                    continue\n",
    "\n",
    "                # Final check for NaN values before similarity calculation\n",
    "                if np.isnan(embeddings).any():\n",
    "                    print(f\"NaN values in embeddings after processing, fixing...\")\n",
    "                    embeddings = np.nan_to_num(embeddings, nan=0.0)\n",
    "                \n",
    "                # Compute similarity matrix\n",
    "                similarity_matrix = compute_cosine_similarity(embeddings)\n",
    "                \n",
    "                # Verify the matrix properties\n",
    "                issues = []\n",
    "                if np.isnan(similarity_matrix).any():\n",
    "                    print(f\"NaN values in similarity matrix, fixing...\")\n",
    "                    similarity_matrix = np.nan_to_num(similarity_matrix, nan=0.0)\n",
    "                    issues.append(\"Contains NaN (fixed)\")\n",
    "                \n",
    "                if not np.allclose(similarity_matrix, similarity_matrix.T, rtol=1e-5, atol=1e-8):\n",
    "                    print(f\"Matrix not symmetric, enforcing symmetry...\")\n",
    "                    similarity_matrix = (similarity_matrix + similarity_matrix.T) / 2\n",
    "                    issues.append(\"Matrix not symmetric (fixed)\")\n",
    "                \n",
    "                if not np.allclose(np.diag(similarity_matrix), np.ones(similarity_matrix.shape[0]), rtol=1e-5, atol=1e-8):\n",
    "                    print(f\"Diagonal values not 1, fixing...\")\n",
    "                    np.fill_diagonal(similarity_matrix, 1.0)\n",
    "                    issues.append(\"Diagonal values not 1 (fixed)\")\n",
    "\n",
    "                # Save as CSV\n",
    "                df = pd.DataFrame(similarity_matrix, index=nodes, columns=nodes)\n",
    "                output_path = os.path.join(OUTPUT_DIR, f\"{file.replace('.gml', '_cosine_similarity.csv')}\")\n",
    "                df.to_csv(output_path)\n",
    "                \n",
    "                status = \"Saved\" if not issues else f\"Saved with fixes: {', '.join(issues)}\"\n",
    "                print(f\"{status} {output_path}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "print(\"\\n Spectral embeddings and cosine similarity computed for all small networks!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checking ADV_train_0_cosine_similarity.csv...\n",
      "🔍 Checking ADV_train_1_cosine_similarity.csv...\n",
      "🔍 Checking ADV_train_2_cosine_similarity.csv...\n",
      "🔍 Checking ADV_train_3_cosine_similarity.csv...\n",
      "🔍 Checking ADV_train_4_cosine_similarity.csv...\n",
      "🔍 Checking BUP_train_0_cosine_similarity.csv...\n",
      "🔍 Checking BUP_train_1_cosine_similarity.csv...\n",
      "🔍 Checking BUP_train_2_cosine_similarity.csv...\n",
      "🔍 Checking BUP_train_3_cosine_similarity.csv...\n",
      "🔍 Checking BUP_train_4_cosine_similarity.csv...\n",
      "🔍 Checking CDM_train_0_cosine_similarity.csv...\n",
      "🔍 Checking CDM_train_1_cosine_similarity.csv...\n",
      "🔍 Checking CDM_train_2_cosine_similarity.csv...\n",
      "🔍 Checking CDM_train_3_cosine_similarity.csv...\n",
      "🔍 Checking CDM_train_4_cosine_similarity.csv...\n",
      "🔍 Checking CEG_train_0_cosine_similarity.csv...\n",
      "🔍 Checking CEG_train_1_cosine_similarity.csv...\n",
      "🔍 Checking CEG_train_2_cosine_similarity.csv...\n",
      "🔍 Checking CEG_train_3_cosine_similarity.csv...\n",
      "🔍 Checking CEG_train_4_cosine_similarity.csv...\n",
      "🔍 Checking CGS_train_0_cosine_similarity.csv...\n",
      "🔍 Checking CGS_train_1_cosine_similarity.csv...\n",
      "🔍 Checking CGS_train_2_cosine_similarity.csv...\n",
      "🔍 Checking CGS_train_3_cosine_similarity.csv...\n",
      "🔍 Checking CGS_train_4_cosine_similarity.csv...\n",
      "🔍 Checking EML_train_0_cosine_similarity.csv...\n",
      "🔍 Checking EML_train_1_cosine_similarity.csv...\n",
      "🔍 Checking EML_train_2_cosine_similarity.csv...\n",
      "🔍 Checking EML_train_3_cosine_similarity.csv...\n",
      "🔍 Checking EML_train_4_cosine_similarity.csv...\n",
      "🔍 Checking ERD_train_0_cosine_similarity.csv...\n",
      "🔍 Checking ERD_train_1_cosine_similarity.csv...\n",
      "🔍 Checking ERD_train_2_cosine_similarity.csv...\n",
      "🔍 Checking ERD_train_3_cosine_similarity.csv...\n",
      "🔍 Checking ERD_train_4_cosine_similarity.csv...\n",
      "🔍 Checking FBK_train_0_cosine_similarity.csv...\n",
      "🔍 Checking FBK_train_1_cosine_similarity.csv...\n",
      "🔍 Checking FBK_train_2_cosine_similarity.csv...\n",
      "🔍 Checking FBK_train_3_cosine_similarity.csv...\n",
      "🔍 Checking FBK_train_4_cosine_similarity.csv...\n",
      "🔍 Checking GRQ_train_0_cosine_similarity.csv...\n",
      "🔍 Checking GRQ_train_1_cosine_similarity.csv...\n",
      "🔍 Checking GRQ_train_2_cosine_similarity.csv...\n",
      "🔍 Checking GRQ_train_3_cosine_similarity.csv...\n",
      "🔍 Checking GRQ_train_4_cosine_similarity.csv...\n",
      "🔍 Checking HMT_train_0_cosine_similarity.csv...\n",
      "🔍 Checking HMT_train_1_cosine_similarity.csv...\n",
      "🔍 Checking HMT_train_2_cosine_similarity.csv...\n",
      "🔍 Checking HMT_train_3_cosine_similarity.csv...\n",
      "🔍 Checking HMT_train_4_cosine_similarity.csv...\n",
      "🔍 Checking HPD_train_0_cosine_similarity.csv...\n",
      "🔍 Checking HPD_train_1_cosine_similarity.csv...\n",
      "🔍 Checking HPD_train_2_cosine_similarity.csv...\n",
      "🔍 Checking HPD_train_3_cosine_similarity.csv...\n",
      "🔍 Checking HPD_train_4_cosine_similarity.csv...\n",
      "🔍 Checking HTC_train_0_cosine_similarity.csv...\n",
      "🔍 Checking HTC_train_1_cosine_similarity.csv...\n",
      "🔍 Checking HTC_train_2_cosine_similarity.csv...\n",
      "🔍 Checking HTC_train_3_cosine_similarity.csv...\n",
      "🔍 Checking HTC_train_4_cosine_similarity.csv...\n",
      "🔍 Checking INF_train_0_cosine_similarity.csv...\n",
      "🔍 Checking INF_train_1_cosine_similarity.csv...\n",
      "🔍 Checking INF_train_2_cosine_similarity.csv...\n",
      "🔍 Checking INF_train_3_cosine_similarity.csv...\n",
      "🔍 Checking INF_train_4_cosine_similarity.csv...\n",
      "🔍 Checking KHN_train_0_cosine_similarity.csv...\n",
      "🔍 Checking KHN_train_1_cosine_similarity.csv...\n",
      "🔍 Checking KHN_train_2_cosine_similarity.csv...\n",
      "🔍 Checking KHN_train_3_cosine_similarity.csv...\n",
      "🔍 Checking KHN_train_4_cosine_similarity.csv...\n",
      "🔍 Checking LDG_train_0_cosine_similarity.csv...\n",
      "🔍 Checking LDG_train_1_cosine_similarity.csv...\n",
      "🔍 Checking LDG_train_2_cosine_similarity.csv...\n",
      "🔍 Checking LDG_train_3_cosine_similarity.csv...\n",
      "🔍 Checking LDG_train_4_cosine_similarity.csv...\n",
      "🔍 Checking NSC_train_0_cosine_similarity.csv...\n",
      "🔍 Checking NSC_train_1_cosine_similarity.csv...\n",
      "🔍 Checking NSC_train_2_cosine_similarity.csv...\n",
      "🔍 Checking NSC_train_3_cosine_similarity.csv...\n",
      "🔍 Checking NSC_train_4_cosine_similarity.csv...\n",
      "🔍 Checking PGP_train_0_cosine_similarity.csv...\n",
      "🔍 Checking PGP_train_1_cosine_similarity.csv...\n",
      "🔍 Checking PGP_train_2_cosine_similarity.csv...\n",
      "🔍 Checking PGP_train_3_cosine_similarity.csv...\n",
      "🔍 Checking PGP_train_4_cosine_similarity.csv...\n",
      "🔍 Checking SMG_train_0_cosine_similarity.csv...\n",
      "🔍 Checking SMG_train_1_cosine_similarity.csv...\n",
      "🔍 Checking SMG_train_2_cosine_similarity.csv...\n",
      "🔍 Checking SMG_train_3_cosine_similarity.csv...\n",
      "🔍 Checking SMG_train_4_cosine_similarity.csv...\n",
      "🔍 Checking UAL_train_0_cosine_similarity.csv...\n",
      "🔍 Checking UAL_train_1_cosine_similarity.csv...\n",
      "🔍 Checking UAL_train_2_cosine_similarity.csv...\n",
      "🔍 Checking UAL_train_3_cosine_similarity.csv...\n",
      "🔍 Checking UAL_train_4_cosine_similarity.csv...\n",
      "🔍 Checking UPG_train_0_cosine_similarity.csv...\n",
      "🔍 Checking UPG_train_1_cosine_similarity.csv...\n",
      "🔍 Checking UPG_train_2_cosine_similarity.csv...\n",
      "🔍 Checking UPG_train_3_cosine_similarity.csv...\n",
      "🔍 Checking UPG_train_4_cosine_similarity.csv...\n",
      "🔍 Checking YST_train_0_cosine_similarity.csv...\n",
      "🔍 Checking YST_train_1_cosine_similarity.csv...\n",
      "🔍 Checking YST_train_2_cosine_similarity.csv...\n",
      "🔍 Checking YST_train_3_cosine_similarity.csv...\n",
      "🔍 Checking YST_train_4_cosine_similarity.csv...\n",
      "🔍 Checking ZWL_train_0_cosine_similarity.csv...\n",
      "🔍 Checking ZWL_train_1_cosine_similarity.csv...\n",
      "🔍 Checking ZWL_train_2_cosine_similarity.csv...\n",
      "🔍 Checking ZWL_train_3_cosine_similarity.csv...\n",
      "🔍 Checking ZWL_train_4_cosine_similarity.csv...\n",
      "\n",
      "🧾 Summary Report:\n",
      "\n",
      "                                  file    mean     std  score  \\\n",
      "0    ADV_train_0_cosine_similarity.csv  0.1467  0.0932     90   \n",
      "80   PGP_train_0_cosine_similarity.csv  0.1137  0.3717     90   \n",
      "79   NSC_train_4_cosine_similarity.csv  0.4504  0.1453     90   \n",
      "78   NSC_train_3_cosine_similarity.csv  0.4613  0.1535     90   \n",
      "77   NSC_train_2_cosine_similarity.csv  0.4508  0.1482     90   \n",
      "..                                 ...     ...     ...    ...   \n",
      "29   EML_train_4_cosine_similarity.csv  0.1730  0.1120     90   \n",
      "53   HPD_train_3_cosine_similarity.csv  0.1088  0.6381     90   \n",
      "109  ZWL_train_4_cosine_similarity.csv  0.2096  0.1173     90   \n",
      "75   NSC_train_0_cosine_similarity.csv  0.4676  0.1577    100   \n",
      "76   NSC_train_1_cosine_similarity.csv  0.4597  0.1526    100   \n",
      "\n",
      "                      issues  \n",
      "0    ⚠️ Values not in [0, 1]  \n",
      "80   ⚠️ Values not in [0, 1]  \n",
      "79   ⚠️ Values not in [0, 1]  \n",
      "78   ⚠️ Values not in [0, 1]  \n",
      "77   ⚠️ Values not in [0, 1]  \n",
      "..                       ...  \n",
      "29   ⚠️ Values not in [0, 1]  \n",
      "53   ⚠️ Values not in [0, 1]  \n",
      "109  ⚠️ Values not in [0, 1]  \n",
      "75                      ✅ OK  \n",
      "76                      ✅ OK  \n",
      "\n",
      "[110 rows x 5 columns]\n",
      "✅ Quality report saved as 'cosine_csv_quality_report.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def validate_cosine_similarity_csv(csv_folder):\n",
    "    results = []\n",
    "\n",
    "    for filename in os.listdir(csv_folder):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(csv_folder, filename)\n",
    "            print(f\"🔍 Checking {filename}...\")\n",
    "\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, index_col=0)\n",
    "                matrix = df.values\n",
    "\n",
    "                issues = []\n",
    "                score = 100\n",
    "\n",
    "                # Check for NaN/Inf\n",
    "                if np.isnan(matrix).any():\n",
    "                    issues.append(\" Contains NaN\")\n",
    "                    score -= 25\n",
    "                if np.isinf(matrix).any():\n",
    "                    issues.append(\" Contains Inf\")\n",
    "                    score -= 25\n",
    "\n",
    "                # Check symmetry\n",
    "                if not np.allclose(matrix, matrix.T, atol=1e-5):\n",
    "                    issues.append(\" Matrix not symmetric\")\n",
    "                    score -= 15\n",
    "\n",
    "                # Check diagonal\n",
    "                diagonal = np.diag(matrix)\n",
    "                if not np.allclose(diagonal, 1.0, atol=1e-2):\n",
    "                    issues.append(\" Diagonal values not 1\")\n",
    "                    score -= 10\n",
    "\n",
    "                # Check range [0, 1]\n",
    "                if matrix.min() < 0 or matrix.max() > 1:\n",
    "                    issues.append(\" Values not in [0, 1]\")\n",
    "                    score -= 10\n",
    "\n",
    "                # Summary stats\n",
    "                mean_val = np.mean(matrix)\n",
    "                std_val = np.std(matrix)\n",
    "\n",
    "                if score == 100:\n",
    "                    issues.append(\" OK\")\n",
    "\n",
    "                results.append({\n",
    "                    \"file\": filename,\n",
    "                    \"mean\": round(mean_val, 4),\n",
    "                    \"std\": round(std_val, 4),\n",
    "                    \"score\": score,\n",
    "                    \"issues\": \"; \".join(issues)\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    \"file\": filename,\n",
    "                    \"mean\": \"N/A\",\n",
    "                    \"std\": \"N/A\",\n",
    "                    \"score\": 0,\n",
    "                    \"issues\": f\" Error reading file: {e}\"\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "csv_folder = \"cosine_training\"  # <- replace with your folder path\n",
    "report_df = validate_cosine_similarity_csv(csv_folder)\n",
    "print(\"\\n Summary Report:\\n\")\n",
    "print(report_df.sort_values(by=\"score\"))\n",
    "\n",
    "# Optionally save the report\n",
    "report_df.to_csv(\"cosine_csv_quality_report.csv\", index=False)\n",
    "print(\" Quality report saved as 'cosine_csv_quality_report.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Found 110 graph files\n",
      "Successfully loaded ADV_train_0.gml with 4907 nodes and 31428 edges\n",
      "Successfully loaded ADV_train_1.gml with 4891 nodes and 31428 edges\n",
      "Successfully loaded ADV_train_2.gml with 4903 nodes and 31428 edges\n",
      "Successfully loaded 110 graphs\n",
      "Maximum feature dimension: 15719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   5%|▌         | 5/100 [12:31<3:53:23, 147.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: Loss = 0.7181, Val AUC = 0.8352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  10%|█         | 10/100 [26:12<4:08:12, 165.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: Loss = 1.3587, Val AUC = 0.8354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  15%|█▌        | 15/100 [43:29<4:46:13, 202.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100: Loss = 0.6920, Val AUC = 0.8354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  15%|█▌        | 15/100 [46:27<4:23:16, 185.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: AUC = 0.8357, AP = 0.8384\n",
      "Model saved to models\\gcn_link_predictor.pt\n",
      "\n",
      "Training completed with Test AUC: 0.8357, Test AP: 0.8384\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from tqdm import trange\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Directories\n",
    "DATA_DIR = \"processed_training_graphs\"       # Input graphs (.gml files)\n",
    "COSINE_SIM_DIR = \"cosine_similarity\"           # (Optional) cosine similarity matrices\n",
    "MODEL_DIR = \"models\"                           # Where to save the trained model\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Define GCN Layer and Model using sparse operations\n",
    "# ---------------------------\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "    def forward(self, x, adj_sparse):\n",
    "        # adj_sparse is a sparse tensor\n",
    "        # Compute normalization: D^{-1/2} A D^{-1/2} should have been precomputed.\n",
    "        support = self.linear(x)\n",
    "        output = torch.sparse.mm(adj_sparse, support)\n",
    "        return output\n",
    "\n",
    "class GCNLinkPredictor(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim, out_dim, dropout=0.2):\n",
    "        super(GCNLinkPredictor, self).__init__()\n",
    "        self.gc1 = GCNLayer(in_features, hidden_dim)\n",
    "        self.gc2 = GCNLayer(hidden_dim, out_dim)\n",
    "        self.dropout = dropout\n",
    "    def encode(self, x, adj_sparse):\n",
    "        x = self.gc1(x, adj_sparse)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj_sparse)\n",
    "        return x\n",
    "    def decode(self, z, edge_index):\n",
    "        src, dst = edge_index\n",
    "        return torch.sum(z[src] * z[dst], dim=1)\n",
    "    def forward(self, x, adj_sparse, edge_index):\n",
    "        z = self.encode(x, adj_sparse)\n",
    "        return self.decode(z, edge_index)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# GraphDataset: Convert dense adjacencies to sparse tensors\n",
    "# -----------------------------------------------------\n",
    "class GraphDataset:\n",
    "    def __init__(self, graph_dir, embedding_dir=None, cosine_dir=None):\n",
    "        self.graph_dir = graph_dir\n",
    "        self.embedding_dir = embedding_dir\n",
    "        self.cosine_dir = cosine_dir\n",
    "        self.graphs = []\n",
    "        self.node_features = []\n",
    "        self.adj_matrices = []  # stores sparse tensors now\n",
    "        self.edge_lists = []\n",
    "        self.train_pos_edges = []\n",
    "        self.val_pos_edges = []\n",
    "        self.val_neg_edges = []\n",
    "        self.test_pos_edges = []\n",
    "        self.test_neg_edges = []\n",
    "        self._load_graphs()\n",
    "    def _load_graphs(self):\n",
    "        graph_files = [f for f in os.listdir(self.graph_dir) if f.endswith(\".gml\")]\n",
    "        print(f\"Found {len(graph_files)} graph files\")\n",
    "        for i, graph_file in enumerate(graph_files):\n",
    "            graph_path = os.path.join(self.graph_dir, graph_file)\n",
    "            try:\n",
    "                G = nx.read_gml(graph_path)\n",
    "                for node in G.nodes():\n",
    "                    G.nodes[node]['original_label'] = str(node)\n",
    "                G = nx.convert_node_labels_to_integers(G, label_attribute='original_label')\n",
    "                # Create dense adjacency and then convert to torch sparse tensor\n",
    "                adj_dense = nx.to_numpy_array(G)\n",
    "                adj_tensor = torch.FloatTensor(adj_dense)\n",
    "                # Convert to sparse: use coalesce to ensure uniqueness of indices\n",
    "                adj_sparse = adj_tensor.to_sparse().coalesce()\n",
    "                num_nodes = len(G.nodes())\n",
    "                # Basic features as identity matrix\n",
    "                node_features = torch.eye(num_nodes)\n",
    "                # (Optional augmentation omitted for brevity)\n",
    "                edges = list(G.edges())\n",
    "                edge_index = torch.tensor([[u, v] for u, v in edges], dtype=torch.long).t()\n",
    "                self._split_edges(edge_index, num_nodes)\n",
    "                self.graphs.append(G)\n",
    "                self.node_features.append(node_features)\n",
    "                self.adj_matrices.append(adj_sparse)\n",
    "                self.edge_lists.append(edge_index)\n",
    "                if i < 3:\n",
    "                    print(f\"Successfully loaded {graph_file} with {num_nodes} nodes and {len(edges)} edges\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {graph_file}: {str(e)}\")\n",
    "    def _split_edges(self, edge_index, num_nodes):\n",
    "        edges = edge_index.t().numpy()\n",
    "        edge_set = set([(int(u), int(v)) for u, v in edges])\n",
    "        num_edges = len(edge_set)\n",
    "        num_val = max(1, int(0.1 * num_edges))\n",
    "        num_test = max(1, int(0.1 * num_edges))\n",
    "        edge_list = list(edge_set)\n",
    "        np.random.shuffle(edge_list)\n",
    "        test_edges = edge_list[:num_test]\n",
    "        val_edges = edge_list[num_test:num_test+num_val]\n",
    "        train_edges = edge_list[num_test+num_val:]\n",
    "        all_edges = set(edge_list + [(v, u) for u, v in edge_list])\n",
    "        non_edges = []\n",
    "        while len(non_edges) < num_val + num_test:\n",
    "            batch_size = max(1000, (num_val + num_test) - len(non_edges))\n",
    "            u_samples = np.random.randint(0, num_nodes, size=batch_size)\n",
    "            v_samples = np.random.randint(0, num_nodes, size=batch_size)\n",
    "            valid = (u_samples != v_samples)\n",
    "            for u, v in zip(u_samples[valid], v_samples[valid]):\n",
    "                if (u, v) not in all_edges and (v, u) not in all_edges:\n",
    "                    non_edges.append((u, v))\n",
    "                    all_edges.add((u, v))\n",
    "                    if len(non_edges) >= num_val + num_test:\n",
    "                        break\n",
    "        val_neg_edges = non_edges[:num_val]\n",
    "        test_neg_edges = non_edges[num_val:num_val+num_test]\n",
    "        self.train_pos_edges.append(torch.tensor(train_edges, dtype=torch.long).t())\n",
    "        self.val_pos_edges.append(torch.tensor(val_edges, dtype=torch.long).t())\n",
    "        self.test_pos_edges.append(torch.tensor(test_edges, dtype=torch.long).t())\n",
    "        self.val_neg_edges.append(torch.tensor(val_neg_edges, dtype=torch.long).t())\n",
    "        self.test_neg_edges.append(torch.tensor(test_neg_edges, dtype=torch.long).t())\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'graph': self.graphs[idx],\n",
    "            'features': self.node_features[idx],\n",
    "            'adj': self.adj_matrices[idx],\n",
    "            'edges': self.edge_lists[idx],\n",
    "            'train_pos': self.train_pos_edges[idx],\n",
    "            'val_pos': self.val_pos_edges[idx],\n",
    "            'val_neg': self.val_neg_edges[idx],\n",
    "            'test_pos': self.test_pos_edges[idx],\n",
    "            'test_neg': self.test_neg_edges[idx]\n",
    "        }\n",
    "\n",
    "def generate_negative_edges(pos_edges, num_nodes, num_samples):\n",
    "    \"\"\"Vectorized negative edge generator.\"\"\"\n",
    "    pos_edge_set = set()\n",
    "    for i in range(pos_edges.shape[1]):\n",
    "        u, v = pos_edges[0, i].item(), pos_edges[1, i].item()\n",
    "        pos_edge_set.add((u, v))\n",
    "        pos_edge_set.add((v, u))\n",
    "    neg_edges = []\n",
    "    while len(neg_edges) < num_samples:\n",
    "        batch_size = max(1000, num_samples - len(neg_edges))\n",
    "        u_samples = np.random.randint(0, num_nodes, size=batch_size)\n",
    "        v_samples = np.random.randint(0, num_nodes, size=batch_size)\n",
    "        valid = (u_samples != v_samples)\n",
    "        for u, v in zip(u_samples[valid], v_samples[valid]):\n",
    "            if (u, v) not in pos_edge_set:\n",
    "                neg_edges.append([u, v])\n",
    "                pos_edge_set.add((u, v))\n",
    "                if len(neg_edges) >= num_samples:\n",
    "                    break\n",
    "    return torch.tensor(neg_edges, dtype=torch.long).t()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Training Function: Single Model for Link Prediction (using sparse adjacencies)\n",
    "# --------------------------------------------------\n",
    "def train_link_prediction(dataset, epochs=100, hidden_dim=128, out_dim=64, lr=0.01, weight_decay=5e-4):\n",
    "    \"\"\"Train a single GCN model for link prediction across the entire dataset and save the model.\"\"\"\n",
    "    if len(dataset) == 0:\n",
    "        raise ValueError(\"No graphs loaded successfully. Please check your data files.\")\n",
    "    max_features = max([data['features'].shape[1] for data in dataset])\n",
    "    print(f\"Maximum feature dimension: {max_features}\")\n",
    "    model = GCNLinkPredictor(max_features, hidden_dim, out_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_auc = 0\n",
    "    best_model_state = None\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    for epoch in trange(1, epochs+1, desc=\"Training epochs\"):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for i in range(len(dataset)):\n",
    "            data = dataset[i]\n",
    "            features = data['features']\n",
    "            if features.shape[1] < max_features:\n",
    "                padding = torch.zeros(features.shape[0], max_features - features.shape[1])\n",
    "                features = torch.cat([features, padding], dim=1)\n",
    "            features = features.to(device)\n",
    "            # Use the sparse adjacency tensor\n",
    "            adj_sparse = data['adj'].to(device)\n",
    "            train_pos = data['train_pos'].to(device)\n",
    "            num_nodes = features.shape[0]\n",
    "            train_neg = generate_negative_edges(train_pos, num_nodes, train_pos.shape[1]).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            z = model.encode(features, adj_sparse)\n",
    "            pos_score = model.decode(z, train_pos)\n",
    "            neg_score = model.decode(z, train_neg)\n",
    "            loss = F.binary_cross_entropy_with_logits(\n",
    "                torch.cat([pos_score, neg_score]),\n",
    "                torch.cat([torch.ones_like(pos_score), torch.zeros_like(neg_score)])\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(dataset)\n",
    "        model.eval()\n",
    "        val_auc = 0\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(dataset)):\n",
    "                data = dataset[i]\n",
    "                features = data['features']\n",
    "                if features.shape[1] < max_features:\n",
    "                    padding = torch.zeros(features.shape[0], max_features - features.shape[1])\n",
    "                    features = torch.cat([features, padding], dim=1)\n",
    "                features = features.to(device)\n",
    "                adj_sparse = data['adj'].to(device)\n",
    "                val_pos = data['val_pos'].to(device)\n",
    "                val_neg = data['val_neg'].to(device)\n",
    "                z = model.encode(features, adj_sparse)\n",
    "                pos_score = model.decode(z, val_pos).cpu().numpy()\n",
    "                neg_score = model.decode(z, val_neg).cpu().numpy()\n",
    "                scores = np.concatenate([pos_score, neg_score])\n",
    "                labels = np.concatenate([np.ones_like(pos_score), np.zeros_like(neg_score)])\n",
    "                val_auc += roc_auc_score(labels, scores)\n",
    "        val_auc /= len(dataset)\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs}: Loss = {avg_loss:.4f}, Val AUC = {val_auc:.4f}\")\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            best_model_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    model.load_state_dict(best_model_state)\n",
    "    model.eval()\n",
    "    test_auc = 0\n",
    "    test_ap = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            data = dataset[i]\n",
    "            features = data['features']\n",
    "            if features.shape[1] < max_features:\n",
    "                padding = torch.zeros(features.shape[0], max_features - features.shape[1])\n",
    "                features = torch.cat([features, padding], dim=1)\n",
    "            features = features.to(device)\n",
    "            adj_sparse = data['adj'].to(device)\n",
    "            test_pos = data['test_pos'].to(device)\n",
    "            test_neg = data['test_neg'].to(device)\n",
    "            z = model.encode(features, adj_sparse)\n",
    "            pos_score = model.decode(z, test_pos).cpu().numpy()\n",
    "            neg_score = model.decode(z, test_neg).cpu().numpy()\n",
    "            scores = np.concatenate([pos_score, neg_score])\n",
    "            labels = np.concatenate([np.ones_like(pos_score), np.zeros_like(neg_score)])\n",
    "            test_auc += roc_auc_score(labels, scores)\n",
    "            test_ap += average_precision_score(labels, scores)\n",
    "    test_auc /= len(dataset)\n",
    "    test_ap /= len(dataset)\n",
    "    print(f\"Test Results: AUC = {test_auc:.4f}, AP = {test_ap:.4f}\")\n",
    "    # Save the model\n",
    "    model_path = os.path.join(MODEL_DIR, \"gcn_link_predictor.pt\")\n",
    "    torch.save({\n",
    "        'state_dict': best_model_state,\n",
    "        'in_features': max_features,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'out_dim': out_dim\n",
    "    }, model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    return model, test_auc, test_ap\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = GraphDataset(DATA_DIR, embedding_dir=None, cosine_dir=COSINE_SIM_DIR)\n",
    "    print(f\"Successfully loaded {len(dataset)} graphs\")\n",
    "    if len(dataset) > 0:\n",
    "        model, test_auc, test_ap = train_link_prediction(\n",
    "            dataset,\n",
    "            epochs=100,\n",
    "            hidden_dim=128,\n",
    "            out_dim=64,\n",
    "            lr=0.01\n",
    "        )\n",
    "        print(f\"\\nTraining completed with Test AUC: {test_auc:.4f}, Test AP: {test_ap:.4f}\")\n",
    "    else:\n",
    "        print(\"No graphs were successfully loaded. Please check your data files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Found 110 graph files\n",
      "Successfully loaded ADV_train_0.gml with 4907 nodes and 31428 edges\n",
      "Successfully loaded ADV_train_1.gml with 4891 nodes and 31428 edges\n",
      "Successfully loaded ADV_train_2.gml with 4903 nodes and 31428 edges\n",
      "Successfully loaded 110 graphs\n",
      "Maximum feature dimension: 15719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   5%|▌         | 5/100 [53:31<19:38:55, 744.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: Loss = 0.6931, Val AUC = 0.6304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  10%|█         | 10/100 [1:39:41<13:19:38, 533.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: Loss = 0.6931, Val AUC = 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  12%|█▏        | 12/100 [1:58:58<14:32:29, 594.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: AUC = 0.5000, AP = 0.5000\n",
      "Model saved to models\\gat_link_predictor.pt\n",
      "\n",
      "Training completed with Test AUC: 0.5000, Test AP: 0.5000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from tqdm import trange\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Directories\n",
    "DATA_DIR = \"processed_training_graphs\"       # Input graphs (.gml files)\n",
    "COSINE_SIM_DIR = \"cosine_similarity\"           # (Optional) cosine similarity matrices\n",
    "MODEL_DIR = \"models\"                           # Where to save the trained model\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Define GAT Layer and Model (using dense adjacency in attention)\n",
    "# ---------------------------\n",
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout=0.6, alpha=0.2, concat=True):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.concat = concat\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "\n",
    "        # Instead of a single parameter \"a\", we create two: one for the source and one for the target.\n",
    "        self.a_l = nn.Parameter(torch.empty(size=(out_features, 1)))\n",
    "        self.a_r = nn.Parameter(torch.empty(size=(out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a_l.data, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.a_r.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        # h: (N, in_features), adj: (N, N) dense adjacency matrix\n",
    "        Wh = torch.mm(h, self.W)  # (N, out_features)\n",
    "        # Compute attention coefficients efficiently:\n",
    "        # (N,1) from source and (N,1) from target, then broadcast sum:\n",
    "        f1 = torch.mm(Wh, self.a_l)  # (N, 1)\n",
    "        f2 = torch.mm(Wh, self.a_r)  # (N, 1)\n",
    "        # Broadcast: add f1 and f2.T to get (N, N) attention scores.\n",
    "        e = self.leakyrelu(f1 + f2.t())\n",
    "        \n",
    "        # Masked attention: set entries where there's no edge to a huge negative value\n",
    "        zero_vec = -9e15 * torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, Wh)\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "\n",
    "    def _prepare_attentional_mechanism_input(self, Wh):\n",
    "        N = Wh.size()[0]\n",
    "        Wh_repeated_in = Wh.repeat_interleave(N, dim=0)\n",
    "        Wh_repeated_out = Wh.repeat(N, 1)\n",
    "        all_combinations = torch.cat([Wh_repeated_in, Wh_repeated_out], dim=1)\n",
    "        return all_combinations.view(N, N, 2 * self.out_features)\n",
    "\n",
    "class GATLinkPredictor(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim, out_dim, dropout=0.6, alpha=0.2):\n",
    "        super(GATLinkPredictor, self).__init__()\n",
    "        # Use one GAT layer with concatenation followed by output layer (without concat)\n",
    "        self.gat1 = GATLayer(in_features, hidden_dim, dropout=dropout, alpha=alpha, concat=True)\n",
    "        self.gat2 = GATLayer(hidden_dim, out_dim, dropout=dropout, alpha=alpha, concat=False)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def encode(self, x, adj_sparse):\n",
    "        # Convert sparse adjacency to dense\n",
    "        adj_dense = adj_sparse.to_dense()\n",
    "        x = self.gat1(x, adj_dense)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gat2(x, adj_dense)\n",
    "        return x\n",
    "\n",
    "    def decode(self, z, edge_index):\n",
    "        src, dst = edge_index\n",
    "        return torch.sum(z[src] * z[dst], dim=1)\n",
    "\n",
    "    def forward(self, x, adj_sparse, edge_index):\n",
    "        z = self.encode(x, adj_sparse)\n",
    "        return self.decode(z, edge_index)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# GraphDataset: Convert dense adjacencies to sparse tensors\n",
    "# -----------------------------------------------------\n",
    "class GraphDataset:\n",
    "    def __init__(self, graph_dir, embedding_dir=None, cosine_dir=None):\n",
    "        self.graph_dir = graph_dir\n",
    "        self.embedding_dir = embedding_dir\n",
    "        self.cosine_dir = cosine_dir\n",
    "        self.graphs = []\n",
    "        self.node_features = []\n",
    "        self.adj_matrices = []  # stores sparse tensors now\n",
    "        self.edge_lists = []\n",
    "        self.train_pos_edges = []\n",
    "        self.val_pos_edges = []\n",
    "        self.val_neg_edges = []\n",
    "        self.test_pos_edges = []\n",
    "        self.test_neg_edges = []\n",
    "        self._load_graphs()\n",
    "\n",
    "    def _load_graphs(self):\n",
    "        graph_files = [f for f in os.listdir(self.graph_dir) if f.endswith(\".gml\")]\n",
    "        print(f\"Found {len(graph_files)} graph files\")\n",
    "        for i, graph_file in enumerate(graph_files):\n",
    "            graph_path = os.path.join(self.graph_dir, graph_file)\n",
    "            try:\n",
    "                G = nx.read_gml(graph_path)\n",
    "                for node in G.nodes():\n",
    "                    G.nodes[node]['original_label'] = str(node)\n",
    "                G = nx.convert_node_labels_to_integers(G, label_attribute='original_label')\n",
    "                # Create dense adjacency and then convert to torch sparse tensor\n",
    "                adj_dense = nx.to_numpy_array(G)\n",
    "                adj_tensor = torch.FloatTensor(adj_dense)\n",
    "                adj_sparse = adj_tensor.to_sparse().coalesce()\n",
    "                num_nodes = len(G.nodes())\n",
    "                # Basic features as identity matrix\n",
    "                node_features = torch.eye(num_nodes)\n",
    "                # (Optional augmentation omitted for brevity)\n",
    "                edges = list(G.edges())\n",
    "                edge_index = torch.tensor([[u, v] for u, v in edges], dtype=torch.long).t()\n",
    "                self._split_edges(edge_index, num_nodes)\n",
    "                self.graphs.append(G)\n",
    "                self.node_features.append(node_features)\n",
    "                self.adj_matrices.append(adj_sparse)\n",
    "                self.edge_lists.append(edge_index)\n",
    "                if i < 3:\n",
    "                    print(f\"Successfully loaded {graph_file} with {num_nodes} nodes and {len(edges)} edges\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {graph_file}: {str(e)}\")\n",
    "\n",
    "    def _split_edges(self, edge_index, num_nodes):\n",
    "        edges = edge_index.t().numpy()\n",
    "        edge_set = set([(int(u), int(v)) for u, v in edges])\n",
    "        num_edges = len(edge_set)\n",
    "        num_val = max(1, int(0.1 * num_edges))\n",
    "        num_test = max(1, int(0.1 * num_edges))\n",
    "        edge_list = list(edge_set)\n",
    "        np.random.shuffle(edge_list)\n",
    "        test_edges = edge_list[:num_test]\n",
    "        val_edges = edge_list[num_test:num_test+num_val]\n",
    "        train_edges = edge_list[num_test+num_val:]\n",
    "        all_edges = set(edge_list + [(v, u) for u, v in edge_list])\n",
    "        non_edges = []\n",
    "        while len(non_edges) < num_val + num_test:\n",
    "            batch_size = max(1000, (num_val + num_test) - len(non_edges))\n",
    "            u_samples = np.random.randint(0, num_nodes, size=batch_size)\n",
    "            v_samples = np.random.randint(0, num_nodes, size=batch_size)\n",
    "            valid = (u_samples != v_samples)\n",
    "            for u, v in zip(u_samples[valid], v_samples[valid]):\n",
    "                if (u, v) not in all_edges and (v, u) not in all_edges:\n",
    "                    non_edges.append((u, v))\n",
    "                    all_edges.add((u, v))\n",
    "                    if len(non_edges) >= num_val + num_test:\n",
    "                        break\n",
    "        val_neg_edges = non_edges[:num_val]\n",
    "        test_neg_edges = non_edges[num_val:num_val+num_test]\n",
    "        self.train_pos_edges.append(torch.tensor(train_edges, dtype=torch.long).t())\n",
    "        self.val_pos_edges.append(torch.tensor(val_edges, dtype=torch.long).t())\n",
    "        self.test_pos_edges.append(torch.tensor(test_edges, dtype=torch.long).t())\n",
    "        self.val_neg_edges.append(torch.tensor(val_neg_edges, dtype=torch.long).t())\n",
    "        self.test_neg_edges.append(torch.tensor(test_neg_edges, dtype=torch.long).t())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'graph': self.graphs[idx],\n",
    "            'features': self.node_features[idx],\n",
    "            'adj': self.adj_matrices[idx],\n",
    "            'edges': self.edge_lists[idx],\n",
    "            'train_pos': self.train_pos_edges[idx],\n",
    "            'val_pos': self.val_pos_edges[idx],\n",
    "            'val_neg': self.val_neg_edges[idx],\n",
    "            'test_pos': self.test_pos_edges[idx],\n",
    "            'test_neg': self.test_neg_edges[idx]\n",
    "        }\n",
    "\n",
    "def generate_negative_edges(pos_edges, num_nodes, num_samples):\n",
    "    \"\"\"Vectorized negative edge generator.\"\"\"\n",
    "    pos_edge_set = set()\n",
    "    for i in range(pos_edges.shape[1]):\n",
    "        u, v = pos_edges[0, i].item(), pos_edges[1, i].item()\n",
    "        pos_edge_set.add((u, v))\n",
    "        pos_edge_set.add((v, u))\n",
    "    neg_edges = []\n",
    "    while len(neg_edges) < num_samples:\n",
    "        batch_size = max(1000, num_samples - len(neg_edges))\n",
    "        u_samples = np.random.randint(0, num_nodes, size=batch_size)\n",
    "        v_samples = np.random.randint(0, num_nodes, size=batch_size)\n",
    "        valid = (u_samples != v_samples)\n",
    "        for u, v in zip(u_samples[valid], v_samples[valid]):\n",
    "            if (u, v) not in pos_edge_set:\n",
    "                neg_edges.append([u, v])\n",
    "                pos_edge_set.add((u, v))\n",
    "                if len(neg_edges) >= num_samples:\n",
    "                    break\n",
    "    return torch.tensor(neg_edges, dtype=torch.long).t()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Training Function: Single Model for Link Prediction using GAT\n",
    "# --------------------------------------------------\n",
    "def train_link_prediction(dataset, epochs=100, hidden_dim=128, out_dim=64, lr=0.01, weight_decay=5e-4):\n",
    "    \"\"\"Train a single GAT model for link prediction across the entire dataset and save the model.\"\"\"\n",
    "    if len(dataset) == 0:\n",
    "        raise ValueError(\"No graphs loaded successfully. Please check your data files.\")\n",
    "    max_features = max([data['features'].shape[1] for data in dataset])\n",
    "    print(f\"Maximum feature dimension: {max_features}\")\n",
    "    model = GATLinkPredictor(max_features, hidden_dim, out_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_auc = 0\n",
    "    best_model_state = None\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    for epoch in trange(1, epochs+1, desc=\"Training epochs\"):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for i in range(len(dataset)):\n",
    "            data = dataset[i]\n",
    "            features = data['features']\n",
    "            if features.shape[1] < max_features:\n",
    "                padding = torch.zeros(features.shape[0], max_features - features.shape[1])\n",
    "                features = torch.cat([features, padding], dim=1)\n",
    "            features = features.to(device)\n",
    "            # Use the sparse adjacency tensor\n",
    "            adj_sparse = data['adj'].to(device)\n",
    "            train_pos = data['train_pos'].to(device)\n",
    "            num_nodes = features.shape[0]\n",
    "            train_neg = generate_negative_edges(train_pos, num_nodes, train_pos.shape[1]).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            z = model.encode(features, adj_sparse)\n",
    "            pos_score = model.decode(z, train_pos)\n",
    "            neg_score = model.decode(z, train_neg)\n",
    "            loss = F.binary_cross_entropy_with_logits(\n",
    "                torch.cat([pos_score, neg_score]),\n",
    "                torch.cat([torch.ones_like(pos_score), torch.zeros_like(neg_score)])\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(dataset)\n",
    "        model.eval()\n",
    "        val_auc = 0\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(dataset)):\n",
    "                data = dataset[i]\n",
    "                features = data['features']\n",
    "                if features.shape[1] < max_features:\n",
    "                    padding = torch.zeros(features.shape[0], max_features - features.shape[1])\n",
    "                    features = torch.cat([features, padding], dim=1)\n",
    "                features = features.to(device)\n",
    "                adj_sparse = data['adj'].to(device)\n",
    "                val_pos = data['val_pos'].to(device)\n",
    "                val_neg = data['val_neg'].to(device)\n",
    "                z = model.encode(features, adj_sparse)\n",
    "                pos_score = model.decode(z, val_pos).cpu().numpy()\n",
    "                neg_score = model.decode(z, val_neg).cpu().numpy()\n",
    "                scores = np.concatenate([pos_score, neg_score])\n",
    "                labels = np.concatenate([np.ones_like(pos_score), np.zeros_like(neg_score)])\n",
    "                val_auc += roc_auc_score(labels, scores)\n",
    "        val_auc /= len(dataset)\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs}: Loss = {avg_loss:.4f}, Val AUC = {val_auc:.4f}\")\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            best_model_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    model.load_state_dict(best_model_state)\n",
    "    model.eval()\n",
    "    test_auc = 0\n",
    "    test_ap = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            data = dataset[i]\n",
    "            features = data['features']\n",
    "            if features.shape[1] < max_features:\n",
    "                padding = torch.zeros(features.shape[0], max_features - features.shape[1])\n",
    "                features = torch.cat([features, padding], dim=1)\n",
    "            features = features.to(device)\n",
    "            adj_sparse = data['adj'].to(device)\n",
    "            test_pos = data['test_pos'].to(device)\n",
    "            test_neg = data['test_neg'].to(device)\n",
    "            z = model.encode(features, adj_sparse)\n",
    "            pos_score = model.decode(z, test_pos).cpu().numpy()\n",
    "            neg_score = model.decode(z, test_neg).cpu().numpy()\n",
    "            scores = np.concatenate([pos_score, neg_score])\n",
    "            labels = np.concatenate([np.ones_like(pos_score), np.zeros_like(neg_score)])\n",
    "            test_auc += roc_auc_score(labels, scores)\n",
    "            test_ap += average_precision_score(labels, scores)\n",
    "    test_auc /= len(dataset)\n",
    "    test_ap /= len(dataset)\n",
    "    print(f\"Test Results: AUC = {test_auc:.4f}, AP = {test_ap:.4f}\")\n",
    "    # Save the model\n",
    "    model_path = os.path.join(MODEL_DIR, \"gat_link_predictor.pt\")\n",
    "    torch.save({\n",
    "        'state_dict': best_model_state,\n",
    "        'in_features': max_features,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'out_dim': out_dim\n",
    "    }, model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    return model, test_auc, test_ap\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = GraphDataset(DATA_DIR, embedding_dir=None, cosine_dir=COSINE_SIM_DIR)\n",
    "    print(f\"Successfully loaded {len(dataset)} graphs\")\n",
    "    if len(dataset) > 0:\n",
    "        model, test_auc, test_ap = train_link_prediction(\n",
    "            dataset,\n",
    "            epochs=100,\n",
    "            hidden_dim=128,\n",
    "            out_dim=64,\n",
    "            lr=0.01\n",
    "        )\n",
    "        print(f\"\\nTraining completed with Test AUC: {test_auc:.4f}, Test AP: {test_ap:.4f}\")\n",
    "    else:\n",
    "        print(\"No graphs were successfully loaded. Please check your data files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Found 110 graph files\n",
      "Successfully loaded ADV_train_0.gml with 4907 nodes and 31428 edges\n",
      "Successfully loaded ADV_train_1.gml with 4891 nodes and 31428 edges\n",
      "Successfully loaded ADV_train_2.gml with 4903 nodes and 31428 edges\n",
      "Successfully loaded 110 graphs\n",
      "Maximum feature dimension: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   5%|▌         | 5/100 [05:18<1:42:16, 64.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: Loss = 0.6259, Val AUC = 0.7568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  10%|█         | 10/100 [10:22<1:36:37, 64.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: Loss = 0.5915, Val AUC = 0.7823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  15%|█▌        | 15/100 [15:41<1:29:18, 63.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100: Loss = 0.5941, Val AUC = 0.8091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  20%|██        | 20/100 [19:45<1:10:00, 52.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100: Loss = 0.5874, Val AUC = 0.8182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  21%|██        | 21/100 [21:09<1:19:37, 60.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: AUC = 0.7938, AP = 0.8017\n",
      "Model saved to models\\graphsage_link_predictor.pt\n",
      "\n",
      "Training completed with Test AUC: 0.7938, Test AP: 0.8017\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from tqdm import trange\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Directories\n",
    "DATA_DIR = \"processed_training_graphs\"       # Input graphs (.gml files)\n",
    "COSINE_SIM_DIR = \"cosine_similarity\"           # (Optional) cosine similarity matrices\n",
    "MODEL_DIR = \"models\"                           # Where to save the trained model\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Utility: Create low-dimensional node features using node degree\n",
    "# ------------------------------------------------------------------\n",
    "def get_node_features(G):\n",
    "    # Use the node degree as a 1-dimensional feature per node\n",
    "    degrees = np.array([G.degree(n) for n in G.nodes()], dtype=np.float32)\n",
    "    mean = degrees.mean()\n",
    "    std = degrees.std() if degrees.std() > 0 else 1.0\n",
    "    normalized = (degrees - mean) / std\n",
    "    # Returns a tensor of shape (num_nodes, 1)\n",
    "    return torch.tensor(normalized).unsqueeze(1)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Define GraphSage Layer and GraphSage Link Predictor Model\n",
    "# ------------------------------------------------------------------\n",
    "class GraphSageLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GraphSageLayer, self).__init__()\n",
    "        # Layer takes concatenation of node's own features and the mean of its neighbors' features.\n",
    "        self.linear = nn.Linear(in_features * 2, out_features)\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x, adj_sparse):\n",
    "        # x: (N, in_features); adj_sparse: (N, N) sparse tensor.\n",
    "        # Aggregate neighbors’ features\n",
    "        agg = torch.sparse.mm(adj_sparse, x)\n",
    "        # Compute degree for proper mean aggregation (avoid division by zero)\n",
    "        degrees = torch.sparse.sum(adj_sparse, dim=1).to_dense().unsqueeze(1).clamp(min=1)\n",
    "        mean_agg = agg / degrees\n",
    "        # Concatenate node's own features and aggregated neighborhood features\n",
    "        out = torch.cat([x, mean_agg], dim=1)\n",
    "        out = self.linear(out)\n",
    "        out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "class GraphSageLinkPredictor(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim, out_dim, dropout=0.2):\n",
    "        super(GraphSageLinkPredictor, self).__init__()\n",
    "        self.gs1 = GraphSageLayer(in_features, hidden_dim)\n",
    "        self.gs2 = GraphSageLayer(hidden_dim, out_dim)\n",
    "        self.dropout = dropout\n",
    "    # Passes node features through two GraphSage layers\n",
    "    # and applies dropout between them.\n",
    "    def encode(self, x, adj_sparse):\n",
    "        x = self.gs1(x, adj_sparse)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gs2(x, adj_sparse)\n",
    "        return x\n",
    "    #computes link prediction scores by taking the dot product of node embeddings\n",
    "    def decode(self, z, edge_index):\n",
    "        src, dst = edge_index\n",
    "        return torch.sum(z[src] * z[dst], dim=1)\n",
    "    \n",
    "    def forward(self, x, adj_sparse, edge_index):\n",
    "        z = self.encode(x, adj_sparse)\n",
    "        return self.decode(z, edge_index)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# GraphDataset: Load graphs and generate features/adjacencies and edge splits.\n",
    "# Node features now use get_node_features for lower memory usage.\n",
    "# it loads graphs from the specified directory, generates node features,\n",
    "# and splits edges into training, validation, and test sets.\n",
    "# ------------------------------------------------------------------\n",
    "class GraphDataset:\n",
    "    def __init__(self, graph_dir, embedding_dir=None, cosine_dir=None):\n",
    "        self.graph_dir = graph_dir\n",
    "        self.embedding_dir = embedding_dir\n",
    "        self.cosine_dir = cosine_dir\n",
    "        self.graphs = []\n",
    "        self.node_features = []\n",
    "        self.adj_matrices = []  # Sparse tensors\n",
    "        self.edge_lists = []\n",
    "        self.train_pos_edges = []\n",
    "        self.val_pos_edges = []\n",
    "        self.val_neg_edges = []\n",
    "        self.test_pos_edges = []\n",
    "        self.test_neg_edges = []\n",
    "        self._load_graphs()\n",
    "    #this function loads graphs from the specified directory, generates node features,\n",
    "    def _load_graphs(self):\n",
    "        graph_files = [f for f in os.listdir(self.graph_dir) if f.endswith(\".gml\")]\n",
    "        print(f\"Found {len(graph_files)} graph files\")\n",
    "        for i, graph_file in enumerate(graph_files):\n",
    "            graph_path = os.path.join(self.graph_dir, graph_file)\n",
    "            try:\n",
    "                G = nx.read_gml(graph_path)\n",
    "                for node in G.nodes():\n",
    "                    G.nodes[node]['original_label'] = str(node)\n",
    "                G = nx.convert_node_labels_to_integers(G, label_attribute='original_label')\n",
    "                # Create dense adjacency then convert to a sparse tensor\n",
    "                adj_dense = nx.to_numpy_array(G)\n",
    "                adj_tensor = torch.FloatTensor(adj_dense)\n",
    "                adj_sparse = adj_tensor.to_sparse().coalesce()\n",
    "                num_nodes = len(G.nodes())\n",
    "                # Instead of an identity matrix, use a low-dimensional feature (node degree)\n",
    "                node_features = get_node_features(G)\n",
    "                edges = list(G.edges())\n",
    "                edge_index = torch.tensor([[u, v] for u, v in edges], dtype=torch.long).t()\n",
    "                self._split_edges(edge_index, num_nodes)\n",
    "                self.graphs.append(G)\n",
    "                self.node_features.append(node_features)\n",
    "                self.adj_matrices.append(adj_sparse)\n",
    "                self.edge_lists.append(edge_index)\n",
    "                if i < 3:\n",
    "                    print(f\"Successfully loaded {graph_file} with {num_nodes} nodes and {len(edges)} edges\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {graph_file}: {str(e)}\")\n",
    "    #this function splits edges into training, validation, and test sets.\n",
    "    def _split_edges(self, edge_index, num_nodes):\n",
    "        edges = edge_index.t().numpy()\n",
    "        edge_set = set([(int(u), int(v)) for u, v in edges])\n",
    "        num_edges = len(edge_set)\n",
    "        num_val = max(1, int(0.1 * num_edges))\n",
    "        num_test = max(1, int(0.1 * num_edges))\n",
    "        edge_list = list(edge_set)\n",
    "        np.random.shuffle(edge_list)\n",
    "        test_edges = edge_list[:num_test]\n",
    "        val_edges = edge_list[num_test:num_test+num_val]\n",
    "        train_edges = edge_list[num_test+num_val:]\n",
    "        all_edges = set(edge_list + [(v, u) for u, v in edge_list])\n",
    "        non_edges = []\n",
    "        while len(non_edges) < num_val + num_test:\n",
    "            batch_size = max(1000, (num_val + num_test) - len(non_edges))\n",
    "            u_samples = np.random.randint(0, num_nodes, size=batch_size)\n",
    "            v_samples = np.random.randint(0, num_nodes, size=batch_size)\n",
    "            valid = (u_samples != v_samples)\n",
    "            for u, v in zip(u_samples[valid], v_samples[valid]):\n",
    "                if (u, v) not in all_edges and (v, u) not in all_edges:\n",
    "                    non_edges.append((u, v))\n",
    "                    all_edges.add((u, v))\n",
    "                    if len(non_edges) >= num_val + num_test:\n",
    "                        break\n",
    "        val_neg_edges = non_edges[:num_val]\n",
    "        test_neg_edges = non_edges[num_val:num_val+num_test]\n",
    "        self.train_pos_edges.append(torch.tensor(train_edges, dtype=torch.long).t())\n",
    "        self.val_pos_edges.append(torch.tensor(val_edges, dtype=torch.long).t())\n",
    "        self.test_pos_edges.append(torch.tensor(test_edges, dtype=torch.long).t())\n",
    "        self.val_neg_edges.append(torch.tensor(val_neg_edges, dtype=torch.long).t())\n",
    "        self.test_neg_edges.append(torch.tensor(test_neg_edges, dtype=torch.long).t())\n",
    "    #this function returns the number of graphs in the dataset.\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'graph': self.graphs[idx],\n",
    "            'features': self.node_features[idx],\n",
    "            'adj': self.adj_matrices[idx],\n",
    "            'edges': self.edge_lists[idx],\n",
    "            'train_pos': self.train_pos_edges[idx],\n",
    "            'val_pos': self.val_pos_edges[idx],\n",
    "            'val_neg': self.val_neg_edges[idx],\n",
    "            'test_pos': self.test_pos_edges[idx],\n",
    "            'test_neg': self.test_neg_edges[idx]\n",
    "        }\n",
    "#\n",
    "def generate_negative_edges(pos_edges, num_nodes, num_samples):\n",
    "    pos_edge_set = set()\n",
    "    for i in range(pos_edges.shape[1]):\n",
    "        u, v = pos_edges[0, i].item(), pos_edges[1, i].item()\n",
    "        pos_edge_set.add((u, v))\n",
    "        pos_edge_set.add((v, u))\n",
    "    neg_edges = []\n",
    "    while len(neg_edges) < num_samples:\n",
    "        batch_size = max(1000, num_samples - len(neg_edges))\n",
    "        u_samples = np.random.randint(0, num_nodes, size=batch_size)\n",
    "        v_samples = np.random.randint(0, num_nodes, size=batch_size)\n",
    "        valid = (u_samples != v_samples)\n",
    "        for u, v in zip(u_samples[valid], v_samples[valid]):\n",
    "            if (u, v) not in pos_edge_set:\n",
    "                neg_edges.append([u, v])\n",
    "                pos_edge_set.add((u, v))\n",
    "                if len(neg_edges) >= num_samples:\n",
    "                    break\n",
    "    return torch.tensor(neg_edges, dtype=torch.long).t()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Training Function for Link Prediction using GraphSage\n",
    "# This function trains a GraphSage model for link prediction across the entire dataset.\n",
    "# It uses the node degree as a low-dimensional feature.\n",
    "# The model is trained using binary cross-entropy loss.\n",
    "# The training process includes early stopping based on validation AUC.\n",
    "# The model is saved after training.\n",
    "# ------------------------------------------------------------------\n",
    "def train_link_prediction(dataset, epochs=100, hidden_dim=128, out_dim=64, lr=0.01, weight_decay=5e-4):\n",
    "    if len(dataset) == 0:\n",
    "        raise ValueError(\"No graphs loaded successfully. Please check your data files.\")\n",
    "    max_features = max([data['features'].shape[1] for data in dataset])\n",
    "    print(f\"Maximum feature dimension: {max_features}\")\n",
    "    model = GraphSageLinkPredictor(max_features, hidden_dim, out_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_auc = 0\n",
    "    best_model_state = None\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    for epoch in trange(1, epochs+1, desc=\"Training epochs\"):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for i in range(len(dataset)):\n",
    "            data = dataset[i]\n",
    "            features = data['features']\n",
    "            if features.shape[1] < max_features:\n",
    "                padding = torch.zeros(features.shape[0], max_features - features.shape[1])\n",
    "                features = torch.cat([features, padding], dim=1)\n",
    "            features = features.to(device)\n",
    "            adj_sparse = data['adj'].to(device)\n",
    "            train_pos = data['train_pos'].to(device)\n",
    "            num_nodes = features.shape[0]\n",
    "            train_neg = generate_negative_edges(train_pos, num_nodes, train_pos.shape[1]).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            z = model.encode(features, adj_sparse)\n",
    "            pos_score = model.decode(z, train_pos)\n",
    "            neg_score = model.decode(z, train_neg)\n",
    "            loss = F.binary_cross_entropy_with_logits(\n",
    "                torch.cat([pos_score, neg_score]),\n",
    "                torch.cat([torch.ones_like(pos_score), torch.zeros_like(neg_score)])\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(dataset)\n",
    "        model.eval()\n",
    "        val_auc = 0\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(dataset)):\n",
    "                data = dataset[i]\n",
    "                features = data['features']\n",
    "                if features.shape[1] < max_features:\n",
    "                    padding = torch.zeros(features.shape[0], max_features - features.shape[1])\n",
    "                    features = torch.cat([features, padding], dim=1)\n",
    "                features = features.to(device)\n",
    "                adj_sparse = data['adj'].to(device)\n",
    "                val_pos = data['val_pos'].to(device)\n",
    "                val_neg = data['val_neg'].to(device)\n",
    "                z = model.encode(features, adj_sparse)\n",
    "                pos_score = model.decode(z, val_pos).cpu().numpy()\n",
    "                neg_score = model.decode(z, val_neg).cpu().numpy()\n",
    "                scores = np.concatenate([pos_score, neg_score])\n",
    "                labels = np.concatenate([np.ones_like(pos_score), np.zeros_like(neg_score)])\n",
    "                val_auc += roc_auc_score(labels, scores)\n",
    "        val_auc /= len(dataset)\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs}: Loss = {avg_loss:.4f}, Val AUC = {val_auc:.4f}\")\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            best_model_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    model.load_state_dict(best_model_state)\n",
    "    model.eval()\n",
    "    test_auc = 0\n",
    "    test_ap = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            data = dataset[i]\n",
    "            features = data['features']\n",
    "            if features.shape[1] < max_features:\n",
    "                padding = torch.zeros(features.shape[0], max_features - features.shape[1])\n",
    "                features = torch.cat([features, padding], dim=1)\n",
    "            features = features.to(device)\n",
    "            adj_sparse = data['adj'].to(device)\n",
    "            test_pos = data['test_pos'].to(device)\n",
    "            test_neg = data['test_neg'].to(device)\n",
    "            z = model.encode(features, adj_sparse)\n",
    "            pos_score = model.decode(z, test_pos).cpu().numpy()\n",
    "            neg_score = model.decode(z, test_neg).cpu().numpy()\n",
    "            scores = np.concatenate([pos_score, neg_score])\n",
    "            labels = np.concatenate([np.ones_like(pos_score), np.zeros_like(neg_score)])\n",
    "            test_auc += roc_auc_score(labels, scores)\n",
    "            test_ap += average_precision_score(labels, scores)\n",
    "    test_auc /= len(dataset)\n",
    "    test_ap /= len(dataset)\n",
    "    print(f\"Test Results: AUC = {test_auc:.4f}, AP = {test_ap:.4f}\")\n",
    "    model_path = os.path.join(MODEL_DIR, \"graphsage_link_predictor.pt\")\n",
    "    torch.save({\n",
    "        'state_dict': best_model_state,\n",
    "        'in_features': max_features,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'out_dim': out_dim\n",
    "    }, model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    return model, test_auc, test_ap\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = GraphDataset(DATA_DIR, embedding_dir=None, cosine_dir=COSINE_SIM_DIR)\n",
    "    print(f\"Successfully loaded {len(dataset)} graphs\")\n",
    "    if len(dataset) > 0:\n",
    "        model, test_auc, test_ap = train_link_prediction(\n",
    "            dataset,\n",
    "            epochs=100,\n",
    "            hidden_dim=128,\n",
    "            out_dim=64,\n",
    "            lr=0.01\n",
    "        )\n",
    "        print(f\"\\nTraining completed with Test AUC: {test_auc:.4f}, Test AP: {test_ap:.4f}\")\n",
    "    else:\n",
    "        print(\"No graphs were successfully loaded. Please check your data files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:20<00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessing Complete!\n",
      "Processed graphs are stored in: processed_full_graphs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Directories\n",
    "TRAINING_DIR = \"Full_data\"  # Directory containing training .net files\n",
    "PROCESSED_DIR = \"processed_full_graphs\"  # Where preprocessed graphs (.gml) will be stored\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "### **Step 1: Convert .net to .gml and Preprocess Graphs**\n",
    "def preprocess_graph(file_path, output_path):\n",
    "    \"\"\"Converts .net to .gml, removes isolated nodes, and normalizes labels.\"\"\"\n",
    "    G = nx.read_pajek(file_path)  # Load .net graph\n",
    "    G = nx.Graph(G)  # Convert to undirected (if needed)\n",
    "    G.remove_nodes_from(list(nx.isolates(G)))  # Remove isolated nodes\n",
    "\n",
    "    # Convert node labels to integers (required for embeddings)\n",
    "    G = nx.convert_node_labels_to_integers(G, label_attribute=\"original_label\")\n",
    "\n",
    "    # Save as .gml for better compatibility\n",
    "    nx.write_gml(G, output_path)\n",
    "\n",
    "### **Step 2: Process All Graphs in Training Data**\n",
    "for file in tqdm(os.listdir(TRAINING_DIR)):\n",
    "    if file.endswith(\".net\"):\n",
    "        file_path = os.path.join(TRAINING_DIR, file)\n",
    "        output_file = os.path.join(PROCESSED_DIR, file.replace(\".net\", \".gml\"))\n",
    "\n",
    "        # Convert & Preprocess\n",
    "        preprocess_graph(file_path, output_file)\n",
    "\n",
    "print(\" Preprocessing Complete!\")\n",
    "print(f\"Processed graphs are stored in: {PROCESSED_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "graph_predictions.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"750px\"\n",
       "            src=\"graph_predictions.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2a6ef7533e0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Paths to the graphs – adjust as needed.\n",
    "original_graph_path = os.path.join(\"processed_testing_graphs\", \"CEG_test_0.gml\")\n",
    "predicted_graph_path = os.path.join(\"processed_testing_graphs\", \"CEG_test_0_with_predictions_gcn.gml\")\n",
    "\n",
    "# Load graphs with networkx.\n",
    "G_orig = nx.read_gml(original_graph_path)\n",
    "G_pred = nx.read_gml(predicted_graph_path)\n",
    "\n",
    "# Identify predicted edges not in the original graph.\n",
    "original_edges = set(G_orig.edges())\n",
    "predicted_edges = set(G_pred.edges())\n",
    "new_predicted_edges = predicted_edges - original_edges\n",
    "\n",
    "# Create a PyVis network.\n",
    "net = Network(height=\"750px\", width=\"100%\", bgcolor=\"white\", font_color=\"black\", notebook=True)\n",
    "net.force_atlas_2based()\n",
    "net.barnes_hut()\n",
    "\n",
    "# Add all nodes from the original graph.\n",
    "for node in G_orig.nodes():\n",
    "    net.add_node(node, label=str(node))\n",
    "\n",
    "# Add original edges (blue, solid).\n",
    "for u, v in original_edges:\n",
    "    net.add_edge(u, v, color=\"blue\")\n",
    "\n",
    "# Add new predicted edges (red, dashed).\n",
    "for u, v in new_predicted_edges:\n",
    "    net.add_edge(u, v, color=\"green\", dashes=True)\n",
    "\n",
    "# Save and show the visualization.\n",
    "net.show(\"graph_predictions.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives (TP): 430\n",
      "False Positives (FP): 15\n",
      "False Negatives (FN): 0\n",
      "\n",
      "Precision: 0.966\n",
      "Recall: 1.000\n",
      "F1 Score: 0.983\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Load graphs from GML files\n",
    "original_graph = nx.read_gml('processed_testing_graphs/CEG_test_0.gml')\n",
    "predicted_graph = nx.read_gml('processed_testing_graphs/CEG_test_0_with_predictions.gml')\n",
    "\n",
    "# Convert edge lists to sets of sorted tuples\n",
    "original_edges = set(tuple(sorted(edge)) for edge in original_graph.edges())\n",
    "predicted_edges = set(tuple(sorted(edge)) for edge in predicted_graph.edges())\n",
    "\n",
    "# Comparison\n",
    "true_positives = predicted_edges & original_edges\n",
    "false_positives = predicted_edges - original_edges\n",
    "false_negatives = original_edges - predicted_edges\n",
    "\n",
    "# Accuracy Metrics\n",
    "precision = len(true_positives) / len(predicted_edges) if predicted_edges else 0\n",
    "recall = len(true_positives) / len(original_edges) if original_edges else 0\n",
    "f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) else 0\n",
    "\n",
    "# Output\n",
    "print(f\"True Positives (TP): {len(true_positives)}\")\n",
    "print(f\"False Positives (FP): {len(false_positives)}\")\n",
    "print(f\"False Negatives (FN): {len(false_negatives)}\\n\")\n",
    "\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1 Score: {f1_score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
